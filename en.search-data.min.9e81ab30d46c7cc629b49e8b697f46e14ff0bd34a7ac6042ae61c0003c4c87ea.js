'use strict';(function(){const indexCfg={cache:true};indexCfg.doc={id:'id',field:['title','content'],store:['title','href','section'],};const index=FlexSearch.create('balance',indexCfg);window.bookSearchIndex=index;index.add({'id':0,'href':'/MLOS/build/uncrustify/','title':"Uncrustify",'section':"Builds",'content':"Uncrustify This directory contains the uncrustify config used to enforce C++ code style.\nWe follow something close to the Google C++ style guidelines with some minor changes to match Microsoft\u0026rsquo;s SqlServer coding style.\nWe additionally include recent copies of the uncrustify binaries for Windows.\nUncrustify can be disabled temporarily by setting the UncrustifyEnabled=false property (or via environment variables) for msbuild.\nAlternatively, one can enable \u0026ldquo;check only\u0026rdquo; mode by setting UncrustifyAutoFix=false.\nSee Also:\n build/Mlos.Cpp.targets build/Uncrustify.targets  "});index.add({'id':1,'href':'/MLOS/CODE_OF_CONDUCT/','title':"C O D E O F C O N D U C T",'section':"",'content':"Microsoft Open Source Code of Conduct This project has adopted the Microsoft Open Source Code of Conduct.\nResources:\n Microsoft Open Source Code of Conduct Microsoft Code of Conduct FAQ Contact opencode@microsoft.com with questions or concerns  "});index.add({'id':2,'href':'/MLOS/CONTRIBUTING/','title':"C O N T R I B U T I N G",'section':"",'content':"Contributing to MLOS This project welcomes contributions and suggestions. Most contributions require you to agree to a Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.\nWhen you submit a pull request, a CLA bot will automatically determine whether you need to provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repos using our CLA.\nThis project has adopted the Microsoft Open Source Code of Conduct. For more information see the Code of Conduct FAQ or contact opencode@microsoft.com with any additional questions or comments.\nDetails main is considered the primary development branch.\nWe expect development to follow a typical \u0026ldquo;gitflow\u0026rdquo; style workflow:\n  Fork a copy of the MLOS repo in Github.\n  Create a development (a.k.a. topic) branch off of main to work on changes.\ngit checkout -b YourDevName/some-topic-description main\r  Submit changes for inclusion as a Pull Request on Github.\n  PRs are associated with Github Issues and need MLOS-committers to sign-off (in addition to other CI pipeline checks like tests and lint checks to pass).\n  Once approved, the PR can be completed using a squash merge in order to keep a nice linear history.\n  Caveats There are consumers of MLOS internal to Microsoft that use an internal copy of the Github repo targetting code that is not open-sourced. This arrangement sometimes means porting changes from the internal repo to Github (and vise-versa). When that happens, the changes are submitted as a PR as described above, with the slight modification of (once approved and passing tests) using a rebase based merge instead of a squash merge in order to allow detecting duplicate patches between the public and private repos.\nAdditionally, to try and catch breaking changes we run some extra internal integration tests as well. If they do find issues, we encourage a conversation to occur on how to resolve them in the PRs.\n"});index.add({'id':3,'href':'/MLOS/documentation/01-Prerequisites/','title':"01 Prerequisites",'section':"Documentation",'content':"Prerequisites for building and using MLOS These are one-time setup instructions that should be executed prior to following the build instructions in 02-Build.md\nContents  Prerequisites for building and using MLOS  Contents Clone the repository Python quickstart Linux  Linux Distribution Requirements Option 1: Linux Docker Build Env  Install Docker Build the Docker Image  Pull the upstream docker image Local docker image build     Option 2: Manual Build Tools Install Install Python on Linux  Option 1: Docker Python Install Option 2: Using Conda     Windows  Step 1: Install Python Step 2: Install Docker on Windows Step 3: Install Windows Build Tools Step 4: Build the Docker image      MLOS currently supports 64-bit Intel/AMD platforms, though ARM64 support is under development. It supports Windows and Linux environments. Below we provide instructions for each OS.\nClone the repository Make sure you have git installed and clone the repo:\ngit clone https://github.com/microsoft/MLOS.git cd MLOS Python quickstart Some of the examples require only the installation of the mlos Python library, which is easy to install on any operating system.\nIt\u0026rsquo;s recommended to use the Anaconda python distribution. or the smaller miniconda installer. After installing either anaconda or miniconda, you can create a new environment with all requirements for the examples using\nconda env create -f MLOS/source/Mlos.Notebooks/environment.yml The environment will be called mlos_python_environment and you can activate it as follows:\nconda activate mlos_python_environment Use pip to install the Python library:\npip install MLOS/source/Mlos.Python/ After this installation, you can run any of the Python-only example notebooks. To do so you can:\njupyter-notebook --notebook-dir=MLOS/source/Mlos.Notebooks Jupyter will list a few notebooks. A good place to start is the BayesianOptimization.ipynb, which provides an Introduction to Bayesian Optimization.\nLinux On Linux, there are a couple of options to install the build tools and the needed Python environment. The preferred way is via the Docker images. All of them require git and, of course, a Linux installation:\nLinux Distribution Requirements  Ubuntu 16.04 (xenial), 18.04 (bionic), 20.04 (focal)   Other distros/versions may work, but are untested.\n Option 1: Linux Docker Build Env Install Docker Docker is used for certain portions of the end-to-end examples and as a convient way to setup the build/dev/test environments.\n If you are starting with the Python only setup, you can skip this step for now if you wish.\n Please see the official Docker install documenation for distribution specific documentation. The Ubuntu docs are here.\nBuild the Docker Image Pull the upstream docker image docker pull ghcr.io/microsoft-cisl/mlos/mlos-build-ubuntu-20.04 Local docker image build To automatically setup a Linux build environment using docker, run the following to build the image locally:\n# Build the docker image: docker build . --build-arg=UbuntuVersion=20.04 -t mlos-build-ubuntu-20.04 \\  --cache-from ghcr.io/microsoft-cisl/mlos/mlos-build-ubuntu-20.04  Where 20.04 can also be replaced with another supported UbuntuVersion.\nNote: in Linux environments, you can also simply execute make docker-image\nSee the Makefile for advanced usage details.\n See 02-Build.md for instructions on how to run this image.\nOption 2: Manual Build Tools Install To manually setup your own Linux build environment:\n# Make sure some basic build tools are available: sudo apt-get install build-essential # Make sure some apt related tools are available: sudo apt-get install \\  apt-transport-https ca-certificates curl gnupg-agent software-properties-common # Make sure an appropriate version of clang is available: ./scripts/install.llvm-clang.sh # Make sure some dotnet dependencies are available: sudo apt-get install liblttng-ctl0 liblttng-ust0 zlib1g libxml2  Note: older distros such as Ubuntu 16.04 may also need the libcurl3 package installed for dotnet restore to work, but is unavailable on (or will break) more recent versions of Ubuntu.\nNote: libxml2 pulls an appropriate version of libicu.\n # Install dotnet in the system: ./scripts/install.dotnet.sh # Install cmake in the system: ./scripts/install.cmake.sh Optional tools:\nsudo apt-get install exuberant-ctags  When available make ctags can be invoked to help generate a tags database at the root of the source tree to allow easier code navigation in editors that support it.\n Install Python on Linux Option 1: Docker Python Install If you used the Docker build image instructions you\u0026rsquo;re done! All of the required packages should already be installed in the image.\nOption 2: Using Conda Follow the Python Quickstart above.\nWindows MLOS is easiest to use on Windows 10, Version 1903 (March 2019) or newer.\nStep 1: Install Python Follow the Python Quickstart above.\nStep 2: Install Docker on Windows Portions of MLOS use Docker. Please follow the instructions on the Docker Website to install it. Note that on Windows Home, you need a fairly recent Windows version to install Docker (Windows 10 1903 or newer).\nOn Windows 10 v1903 or newer, we recommend you use the Windows Subsytem for Linux v2 to run the containers. On older Windows 10, you can resort to the Hyper-V support of Docker.\nStep 3: Install Windows Build Tools Download and install Visual Studio 2019 (free) Community Edition:\nhttps://visualstudio.microsoft.com/vs/community/\nBe sure to include support for .Net Core and C++.\nStep 4: Build the Docker image The instructions for building the docker image are the same as for Linux.\n"});index.add({'id':4,'href':'/MLOS/documentation/02-Build/','title':"02 Build",'section':"Documentation",'content':"Build Instructions for MLOS Prerequisites See 01-Prerequisites.md for initial build tools setup instructions.\nThere are different instructions according to the environment setup you chose.\nContents  Build Instructions for MLOS  Prerequisites Contents Docker  Create a new container instance  Using the upstream container image Using the locally built image   Other useful docker commands Start an existing container instance Get a new shell in a running container instance   Linux  CLI: make VSCode   Windows  CLI: msbuild Building with Visual Studio      Docker If you chose to use the Docker build environment and have already built or pulled a container image using the instructions in 01-Prerequisites.md, then you can start an interactive session using the container image as follows:\nCreate a new container instance Using the upstream container image docker run -it -v $PWD:/src/MLOS \\\r --name mlos-build \\\r ghcr.io/microsoft-cisl/mlos/mlos-build-ubuntu-20.04\rUsing the locally built image # Run the image:\rdocker run -it -v $PWD:/src/MLOS \\\r --name mlos-build \\\r mlos-build-ubuntu-20.04\r Where 20.04 can also be replaced with another supported UbuntuVersion.\nNote: If you receive an error that the container name already exists, then you can use either the docker rm or docker start commands below to retry.\nThe -v $PWD:/src/MLOS option makes the current directory (assumed to be the root of the MLOS repository) available inside the container so that you can edit the code from your host machine, but build it inside the container.\nNote that the build artifacts located at out/ in the container are kept separate by default, so you can test with multiple containers at a time (e.g. each using different Ubuntu versions). You can use additional -v /path/to/out-20.04:/src/MLOS/out style arguments to direct that output to a host accessible locations if desired.\n Other useful docker commands Here are some additional basic docker commands to help manage the container instance.\n# List the MLOS related container instances\rdocker ps -a | grep -i mlos\r# Gracefully stop the container instance\rdocker stop mlos-build\r# Forcefully stop the container instance.\rdocker kill mlos-build\r# Remove the container instance.\rdocker rm mlos-build\rStart an existing container instance # Start the image if it already exists and was stopped:\rdocker start -i mlos-build\rGet a new shell in a running container instance docker exec -it mlos-build /bin/bash\rOnce you have an interactive session in the container, the MLOS source code is available at /src/MLOS and can be built using the same instructions in the Linux: CLI make section below.\nLinux CLI: make We provide Makefile wrappers to invoke the language specific build systems.\nmake\r This is equivalent to make dotnet-build cmake-build\n If you want to switch to a debug build run:\nexport CONFIGURATION=Debug\rmake\r Note: export CONFIGURATION=Release to switch back to Release builds.\n The Makefiles in most source folders are simply wrappers around the cmake build system and allow easier interactive building during development without having to maintain shell environment variables or additional paths.\nIn general cmake is used for C++ projects, with simple CMakeLists.txt wrappers around dotnet build for their C# dependencies to do code generation.\nIn top-level directories you can restrict the build to just dotnet wrappers or just cmake wrappers like so:\nmake dotnet-build\rmake dotnet-test\rmake dotnet-clean\rmake cmake-build\rmake cmake-test\rmake cmake-clean\r Note: A similar shell environment setup can optionally be obtained with the following\nsource ./scripts/init.linux.sh\r To build and run the tests below the current directory run\nmake check\r This is equivalent to make all test\n VSCode TODO\nWindows For the C++ and C# project components, Visual Studio msbuild can be used on Windows systems.\n Note: Visual Studio build tools are available free.\nPlease see the initial setup instructions linked above for details.\n CLI: msbuild To build from the command line on Windows, the Visual Studio build tools need to be added to the shell environment.\n  Setup the powershell environment to find the Visual Studio build tools.\n.\\scripts\\init.windows.ps1\r Note: you can also execute .\\scripts\\init.windows.cmd if you prefer a cmd environment.\n   Use msbuild to build the project file in the current directory.\n e.g. when run from the root of the MLOS repo this will recursively build all the projects and run the tests.\n msbuild /m /r /p:Configuration=Release\rSome additional build flags to help provide additional control over the process:\n /m runs a multi-process parallel build process /r is required on first build and git pull to restore any nuget packages required \\ /fl will optionally produce a msbuild.log file /p:Configuration=Release will perform a non-debug build.  Note: If omitted, msbuild will produce a Debug build by default. Debug builds perform no compiler optimizations, so are useful for troubleshooting, but will be more difficult for MLOS to help optimize.\n  /p:RunUnitTest=false will temporarily skip running unit tests /p:StyleCopEnabled=false will temporarily skip C# style checks /p:UncrustifyEnabled=false will temporarily skip C++ style checks /p:BuildProjectReferences=false will temporarily only build the current project file, and skip rebuilding its dependencies (note: this option doesn\u0026rsquo;t work when building .sln files)    Building with Visual Studio  Note: Visual Studio 2019 Community Edition is available free.\nPlease see the initial setup instructions linked above for details.\n Opening a *.sln file in the source/ directory with Visual Studio 2019 should allow you to build inside the IDE.\n  Setup the shell environment to find the devenv script provided by Visual Studio.\n.\\scripts\\init.windows.ps1\r Note: you can also execute .\\scripts\\init.windows.cmd if you prefer a cmd environment.\n   Launch Visual Studio for a given solution:\ndevenv Mlos.NetCore.sln\rAlternatively, you can launch devenv for a project and manually add its dependencies to the solution that Visual Studio creates. For instance:\ndevenv Mlos.Core.vcxproj\r  "});index.add({'id':5,'href':'/MLOS/documentation/03-ExampleUsage/','title':"03 Example Usage",'section':"Documentation",'content':"Examples of using MLOS to optimize a system TODO\n"});index.add({'id':6,'href':'/MLOS/documentation/04-Test/','title':"04 Test",'section':"Documentation",'content':"Test Instructions for MLOS Contents  Test Instructions for MLOS  Contents Linux Tests  Run C# Tests on Linux Run C++ Tests on Linux Run Python Tests on Linux   Windows  Run C#/C++ Tests on Windows Run Python Tests on Windows      Linux Tests To build and test all of the MLOS code at or below the current folder, regardless of language, run:\nmake check  That is equivalent to make all test\n To only invoke the tests run (not re-check the build):\nmake test See below for additional targets to restrict the languages invoked:\nRun C# Tests on Linux From any source directory, running the following should invoke dotnet build and dotnet test recursively for any projects in that folder or below it:\nmake dotnet-test Run C++ Tests on Linux From any source directory, running the following should invoke approprite cmake and ctest commands to build and test projects recursively in that folder or below it:\nmake cmake-build cmake-test Run Python Tests on Linux First, ensure that the necessary Python modules are installed. See 01-Prerequisites.md for details.\nFrom the root of MLOS source tree:\nmake python-tests invokes scripts/run-python-tests.sh to run the Python unit tests.\nWindows Run C#/C++ Tests on Windows As mentioned in 02-Build.md, invoking msbuild from the tests/ directory will invoke the C++ and C# tests unless the RunUnitTest property is set to false.\nOn Windows, to build and run the tests for both C++ and C#:\ncd test/ msbuild /m /r /p:RunUnitTest=true  Note: Generally RunUnitTest will default to true when left unspecified.\n To only run the tests and not recheck the build:\nmsbuild /p:RunUnitTest=true /p:BuildProjectReferences=false Run Python Tests on Windows First, ensure that the necessary Python modules are installed. See 01-Prerequisites.md for details.\nscripts\\run-python-tests.cmd "});index.add({'id':7,'href':'/MLOS/documentation/05-Debug/','title':"05 Debug",'section':"Documentation",'content':"TODO\n"});index.add({'id':8,'href':'/MLOS/documentation/','title':"Documentation",'section':"",'content':"MLOS Documentation This directory contains project wide documentation for things like coding standards, overall architecture descriptions, build instructions, etc.\nIndividual components may also include their own more detailed documentation within their subdirectories.\n Note: Some documentation uses Mermaid for diagrams in addition to Markdown.\nYou can install Markdown Preview Enhanced for Atom or Visual Code to help render it more easily.\n Getting Started Here\u0026rsquo;s a brief summary of some documentation suggestions to help get started:\n See 01-Prerequisites.md for initial environment setup instructions. See 02-Build.md for basic build instructions. See 03-ExampleUsage.md for some usage examples for applying MLOS to some code/system. See 04-Test.md for notes on testing those changes. See 05-Debug.md for notes on debugging those changes.  Overview Here\u0026rsquo;s some documentation references describing the MLOS architecture:\n MlosArchitecture.md Glossary.md DEEM 2020 Paper  Here\u0026rsquo;s a document describing the layout of the repository:\n RepoOrganization.md  We also have a collection of Troubleshooting Tips started.\nContributing See CONTRIBUTING.md for notes on making changes to MLOS itself.\n"});index.add({'id':9,'href':'/MLOS/documentation/CodingStandard/','title':"Coding Standard",'section':"Documentation",'content':"MLOS Coding Standards MLOS uses and supports multiple languages. Here we document the coding styles and standards we attempt to adhere to and the tools we use to achieve that.\nC++ For C++ we mostly try to follow the Google C++ style guidelines, with a few modifications.\nCurrently we rely on uncrustify to help enforce these rules (plus a little bit of human review).\nSee build/uncrustify/README.md for additional information.\nThough we attempt to make it somewhat readable, we exclude code generated by MLOS from these strict style checks.\nuncrustify is invoked as a part of the build process (see 02-Build.md for details on temporarily disabling it locally).\nC# We use StyleCopAnalyzers to mostly follow the standard recommended C# style guidelines, with a few exceptions listed in build/Mlos.NetCore.ruleset.\nThough we attempt to make it somewhat readable, we exclude code generated by MLOS from these strict style checks using an \u0026lt;auto-generated /\u0026gt; tag file header.\nStyleCop is invoked as a part of the build process (see 02-Build.md for details on temporarily disabling it locally).\nPython We use pylint for Python code to mostly follow PEP 8 guidelines. Its configuration can be found at source/.pylintrc.\nTo run it locally, issues the following commands:\n  One time install of the pylint tool:\npip install pylint\r  Followed by:\n  Linux:\nscripts/run-python-checks.sh\r  Windows:\nscripts\\run-python-checks.cmd\r    We also use licenseheaders to add ensure license headers are added to .py files.\nTo run it locally, issues the following commands:\n  One time install of the licenseheaders tool:\npip install licenseheaders\r  Followed by:\n  Linux:\nscripts/update-python-license-headers.sh\r  Windows:\nscripts\\update-python-license-headers.cmd\r     Note: Currently this has issues with conflicting cross-platform line-ending styles.\nUse git diff --ignore-cr-at-eol to identify the differences.\n "});index.add({'id':10,'href':'/MLOS/documentation/Glossary/','title':"Glossary",'section':"Documentation",'content':"MLOS Terms Glossary TODO\n"});index.add({'id':11,'href':'/MLOS/documentation/MlosArchitecture/','title':"Mlos Architecture",'section':"Documentation",'content':"MLOS Architecture This document provides a brief overview of the MLOS architecture for supporting Machine Learning Optimized Systems.\n MLOS Architecture  High Level Description  Principles Workflows   Architecture Diagram  Main components Shared Memory Regions Target Process  Mlos.Core Shared Channel   Mlos.Agent  Mlos.NetCore Settings registry assemblies Grpc Server Experiment management     Implementation details    High Level Description At a high level, MLOS provides infrastructure to support instance-specific tuning systems software (e.g. written in C++) in a trackable and reproducible way.\nMLOS does this by focusing on optimizing tunables that exist in code.\nTunables can take several forms:\n Build time  e.g. inlined constants controlling buffer size or hash function, choice of concrete ADT implementation, etc.   Startup  e.g. configurations that can only be changed with a process restart   Runtime  e.g. dynamic settings that can changed between instantiations, queries, or other events. Some of these may be known at compile-time, while others may only be known at runtime for a specific instance (e.g. number of tables in a database schema).    To optimize these tunables, the system needs to be observable. In other words, it needs to provide additional data (e.g. workload, telemetry, metrics, etc.) about the its operation. When combined additional information obtained from the system executing the process (e.g. OS/HW metrics), we call this combined set of information the context.\nPrinciples   Separation\nTo support lightweight observable components with minimal impact on the target system\u0026rsquo;s performance, we separate the data aggregation and optimization steps to a (local) external agent process. It communicates with the target system using shared memory channels for low latency.\n  Segmentation\nTo support faster iteration, we allow focusing build-time constant tuning to specific components of the target system using micro benchmarks.\n  Workflows MLOS workflows take roughly two basic forms:\n  Build-time optimization\nMicrobenchmarks develepers write for smart components can be used to explore component tunable parameter values either interactively or in the background (e.g. during continuous integration pipelines).\nThe data collected from those \u0026ldquo;experiments\u0026rdquo; can be captured and explored in Notebook experiences that can then be checked back in with the resulting code change to help support reproducible tests to validate these parameters in the future.\nSince these tunables may affect many instances, optimization goals here may focus on robustness in addition to performance or some other metric.\n  Runtime optimization\nSmart components can be hooked up to receive suggestions from the external agent during runtime for online instance specific optimizations as well.\n  Architecture Diagram Main components   Target process\nThis is the MLOS enabled system to tune (e.g. an end-to-end system like SqlServer or Smart micro benchmarks for a component, etc.). The process contains tunable \u0026ldquo;Smart\u0026rdquo; components that are made observable by the Mlos.Agent by exporting telemetry/metrics over Shared memory channels and are optimized using Mlos.Optimizer.Service suggestions.\n  Mlos.Agent\nThe primary responsibility of the Mlos.Agent is to observe the target process and manage the \u0026ldquo;experiments\u0026rdquo;. The experiments define the aggregates from the telemetry stream and exchange it with Mlos.Optimizer.Service.\n  Mlos.Client\nThe Mlos.Client is a utility application that allows us to query the state of shared components and create new experiments.\n  Shared memory\nShared memory regions are used to exchange messages and share the configuration objects between the Target process and the Mlos.Agent.\n  Mlos.Optimizer.Service\nMlos.Optimizer.Service is a Docker instance containing the Mlos.Optimizer and an RDBMS instance (e.g. SqlServer) for storing experiment data, optimizer models, etc.\nTo exchange the messages between Mlos.Agent experiments and Mlos.Optimizers, the database is currently used as a transport channel.\n  Shared Memory Regions   Global Shared Memory\nThe primary shared memory entry point. It used to bootstrap and contains metadata about all other memory regions. It also includes shared channel synchronization objects.\n  Config Shared Memory\nWe store all components\u0026rsquo; configuration in the config shared memory region. Configuration objects are accessible from the Target process and from Mlos.Agent.\n  Control/Telemetry Channel Shared Memory\nMemory region used (exclusively, no header) to exchange messages from the Target process to Mlos.Agent.\n  Feedback Channel Shared Memory\nA memory region used (exclusively, no header) to exchange messages from Mlos.Agent to Target process.\n  Some memory regions contain a header block that helps with their identification. The exceptions are shared channel memory regions, where the channel uses all memory. Control and Feedback Channels are circular buffers whose size must be a power of two (2N). Prepending a header would prevent proper alignment which is why all communication channel metadata and synchronization objects are located in the Global Shared Memory region.\nSee Also: SharedChannel.md for more details about their implementation.\nTarget Process Mlos.Core Mlos.Core is a (C++) library used inside the target process. It provides an API to handle shared configs and exchange messages with Mlos.Agent via a shared channel.\nMlos.Core contains the following components:\n  MlosContext\nA class responsible for managing shared memory channels. It provides methods for sending telemetry and control messages to the Mlos.Agent as well as receiving feedback messages from it.\n  SharedConfigManager\nA class responsible for managing shared configs. It allows for registering configs in shared memory.\n  Shared Channel  Control Channel Feedback Channel  Mlos.Agent Mlos.NetCore Settings registry assemblies Grpc Server Experiment management Implementation details Shared Memory Channel\n"});index.add({'id':12,'href':'/MLOS/documentation/RepoOrganization/','title':"Repo Organization",'section':"Documentation",'content':"Repo Organization Some notes on the directory layout organization in this repo.\n  There are build files (e.g. dirs.proj for msbuild or dotnet build, or Makefiles for make) in most directories to allow easy recursive building of that subtree you happen to be in.\n Note: we provide Makefile wrappers in most directories to simply help invoke cmake and the Makefiles it generates\n   build/ contains configuration related to building MLOS components\n For instance, .props and .targets files for definining and controlling common msbuild and dotnet build properties and targets are contained there, as are various style check configurations.   Note: For this reason, cmake output is redirected to out/cmake/{Release,Debug}/ instead.\n   source/ contains a directory for each component of MLOS, including unit test source code.\n  i.e. running msbuild or make in the source/ directory will build (and generally analyze) all of the projects, but not necessarily execute their tests.\n  Many components include more detailed documentation about their implementation internals.\nFor instance:\n Mlos Settings System Code Generation System Mlos.Core Shared Memory Communication Channel Mlos.Agent.Server    source/Examples/ contains sample target codes to optimize with the other MLOS components and help describe the integration methods\nFor instance:\n Smart Cache C++      test/ contains a directory and project to invoke each of the unit tests.\n i.e. running msbuild or make in the test/ directory will also run all of the tests.    scripts/ contains some helper scripts to initialize development environments, install tools, invoke build pipelines, run tests, etc.\n  external/ contains code, config, and examples for build integration for use of MLOS with external projects.\nFor instance:\n external/cmake/ contains an MLOS module for use in projects using the cmake build system. ExternalIntegrationExample contains a stripped down example showing how to use it. leveldb contains examples of how to use portions of MLOS to tune LevelDb.    Auto generated content:\n out/ contains most of the intermediate build output, especially for msbuild and dotnet build portions  out/dotnet contains the msbuild and dotnet build outputs (for Windows) out/Mlos.CodeGen.out contains code generation output from each SettingsRegistry project, organized by originating source/ project directory out/Grpc.out contains the output for the grpc messages between the Mlos.Agents   target/ contains final binaries and libraries produced by msbuild and make install that are suitable for execution out/cmake/{Release,Debug}/ contains most of the output from cmake  Note: this is by convention. Though we provide some configurations to help use this path, other tools or IDEs may override it or may need to be configured to work with it.\n  tools/ is where local versions of dotnet, cmake, and cake are fetched and installed.  "});index.add({'id':13,'href':'/MLOS/documentation/Troubleshooting/','title':"Troubleshooting",'section':"Documentation",'content':"Troubleshooting Tips Here are some common tips for troubleshooting various issues.\nContents  Troubleshooting Tips  Contents Editor Integrations  VSCode in WSL  \u0026ldquo;Missing .Net SDK\u0026rdquo; message when executing code . in WSL        Editor Integrations VSCode in WSL \u0026ldquo;Missing .Net SDK\u0026rdquo; message when executing code . in WSL The Omnisharp plugin for VSCode may have trouble finding the dotnet setup locally for the MLOS repo in tools/, even if you source the scripts/init.linux.sh script to setup your PATH environment.\nTo workaround this issue, you can install dotnet system wide for your WSL2 distro.\nHere are the instructions for Ubuntu 20.04:\nhttps://docs.microsoft.com/en-us/dotnet/core/install/linux-ubuntu#2004-\n"});index.add({'id':14,'href':'/MLOS/external/','title':"External",'section':"",'content':"External Integration Code and Examples This directory contains some code, configuration, and examples of how to integrate MLOS with external projects.\nFor instance:\n The cmake directory contains an MLOS module for use in projects using the cmake build system. ExternalIntegrationExample contains a stripped down example showing how to use it. leveldb contains a notebook and other examples of using portions of the MLOS toolkit to tune LevelDb parameters.  "});index.add({'id':15,'href':'/MLOS/external/cmake/','title':"Cmake",'section':"External",'content':"External CMake Modules This directory contains cmake modules for external project integration.\nOverview The general usage pattern is to\n  Use FetchContent to obtain the MLOS source tree as a dependency for the external project\u0026rsquo;s cmake build system and add this directory to their CMAKE_MODULE_PATH so that include(MLOS) can find the MLOS.cmake file in this directory.\nNote: This can happen at a common level for an entire external project\u0026rsquo;s cmake build system.\n# Make MLOS source code available in the external project. include(FetchContent)FetchContent_Declare( mlos GIT_REPOSITORY \u0026#34;https://github.com/microsoft/MLOS\u0026#34; GIT_TAG \u0026#34;main\u0026#34; GIT_SHALLOW OFF )FetchContent_GetProperties(mlos)set(MLOS_CMAKE_BUILD_TYPE Release)if(NOT mlos_POPULATED) FetchContent_Populate(mlos) add_subdirectory(\u0026#34;${mlos_SOURCE_DIR}\u0026#34; \u0026#34;${mlos_BINARY_DIR}\u0026#34; EXCLUDE_FROM_ALL) # Instruct other targets how to find the Mlos cmake module.  set(CMAKE_MODULE_PATH ${CMAKE_MODULE_PATH} \u0026#34;${mlos_SOURCE_DIR}/external/cmake\u0026#34;)endif()# Include the MLOS cmake module (from the path above). # (this includes the add_mlos_settings_registry() function used next) include(MLOS)# Define some common settings used in C# SettingsRegistry and C++ builds. set(MlosCodeGenBaseOutDir \u0026#34;${CMAKE_SOURCE_DIR}/Mlos.CodeGen.out\u0026#34;)include_directories(${MlosCodeGenBaseOutDir})set(MlosSettingsRegistryDllDir \u0026#34;${CMAKE_BINARY_DIR}/SettingsRegistryDlls\u0026#34;)See Also: ExternalIntegrationExample/MlosBuildIntegrations.cmake\n  Use the add_mlos_settings_registry() function provided there to provide a cmake target wrapper for dotnet build of various SmartComponent.SettingsRegistry.csproj files the external project will add.\n# MySmartComponent.SettingsRegistry/CMakeLists.txt: # A wrapper for the MySmartComponent.csproj file in the same directory. include(MLOS)add_mlos_settings_registry( NAME MySmartComponent.SettingsRegistry DIRECTORY \u0026#34;${CMAKE_CURRENT_LIST_DIR}\u0026#34; CODEGEN_OUTPUT_DIR \u0026#34;${MlosCodeGenBaseOutDir}/MySmartComponent\u0026#34; BINPLACE_DIR \u0026#34;${MlosSettingsRegistryDllDir}\u0026#34; USE_LOCAL_MLOS_NUGETS )See Also: ExternalIntegrationExample.SettingsRegistry/CMakeLists.txt\n  Reference those using add_dependencies() in the C/C++ components.\n# MySmartComponent/CMakeLists.txt: # ... existing cmake definitions target_link_libraries(${PROJECT_NAME} Mlos.Core)# SettingsRegistry projects produce C++ codegen artifacts, that this project # consumes, so we mark that project as a dependency. # add_subdirectory(MySmartComponent.SettingsRegistry)add_dependencies(${PROJECT_NAME} MySmartComponent.SettingsRegistry)See Also: ExternalIntegrationExample/CMakeLists.txt\n  "});index.add({'id':16,'href':'/MLOS/external/ExternalIntegrationExample/','title':"External Integration Example",'section':"External",'content':"ExternalIntegrationExample This directory serves as a minimal example for testing integration of MLOS with an external C++ example that uses cmake for its build system.\nIt generally does not depend on anything in the rest of the MLOS repo other than the Mlos.Core header files (both static and codegen outputs) and C# NuGet packages that the main MLOS build can produce.\nSee Also  ExternalIntegrationExample.SettingsRegistry  "});index.add({'id':17,'href':'/MLOS/external/ExternalIntegrationExample/ExternalIntegrationExample.SettingsRegistry/','title':"External Integration Example. Settings Registry",'section':"External Integration Example",'content':"ExternalIntegrationExample.SettingsRegistry This directory contains the SettingsRegistry definitions and build configuration files for the ExternalIntegrationExample project.\n"});index.add({'id':18,'href':'/MLOS/external/leveldb/','title':"Leveldb",'section':"External",'content':"LevelDb Tuning Examples with MLOS This directory contains some examples of using MLOS to tune an external project like LevelDb.\nA rendered notebook can be found here.\n"});index.add({'id':19,'href':'/MLOS/notebooks/','title':"Notebooks",'section':"",'content':"MLOS Sample Notebooks  SmartCacheOptimization SmartCacheCPP LevelDbTuning BayesianOptimization  "});index.add({'id':20,'href':'/MLOS/notebooks/BayesianOptimization/','title':"Bayesian Optimization",'section':"Notebooks",'content':"Download BayesianOptimization.ipynb notebook from IPython.core.display import display, HTML import warnings display(HTML(\u0026#34;\u0026lt;style\u0026gt;.container { width:100% !important; }\u0026lt;/style\u0026gt;\u0026#34;)) warnings.simplefilter(\u0026#34;ignore\u0026#34;) \u0026lt;IPython.core.display.HTML object\u0026gt;  import matplotlib.pyplot as plt import numpy as np import pandas as pd from scipy.stats import t Bayesian Optimization This notebook demonstrates the basic principles of Bayesian Optimization (BO) and how to use MLOS to perform BO.\nMotivation In software performance engineering, the impact different (input) parameters (e.g. buffer size, worker thread count, etc.) can have on the (output) performance of a system for a given workload (input) can be modeled as a multidimensional function - one which we don\u0026rsquo;t know the equation for apriori, but are instead trying to learn through careful sampling of the input space and experimentation (test/benchmark runs) to gather output points. Bayesian optimization is one technique for efficiently selecting the samples in the input space to learn the approximate shape of that function and find its optimum, i.,e. the parameters that lead to the best performance. In this example we use a synthetic (i.e. made-up) function that we can look at directly to stand in for a complex system with unknown characteristics.\nBayesian Optimization is a global optimization strategy, so a way to find the global optimum of a mathematical function that\u0026rsquo;s not necessarily convex. BO is a black-box optimization technique, meaning that it requires only function values and no other information like gradients.\nThis is in contrast to other optimization strategies, such as gradient descent or conjugate gradient that require gradients and are only guaranteed to find a local optimum (if the function is assumed to be convex, this is also the global optimum).\nFinding the global optimum of a general non-convex function is NP-hard, which makes it impossible to provide effective convergence guarantees for any global optimization strategy, including Bayesian Optimization. However, BO has been found to be quite effective in the past.\nA synthetic example Let\u0026rsquo;s take a simple synthetic example of a one-dimensional function that we assume is unknown. If we actually had access to the function, we could use more efficient techniques using calculus and would not be using Bayesian Optimization.\n# define fake performance function # In an actual application, we would not have access to this function directly. # Instead, we could only measure the outcome by running an experiment, such as timing # a particular run of the system. def f(x): return (6*x-2)**2*np.sin(12*x-4) In a real use case for global optimization, the function we want to optimize is usually only implicitly defined and very expensive to compute, such as training and evaluating a neural network, or timing the run of a large workload on a distributed database. Given the cost of evaluating the function, our goal is to find an optimum while keeping the number of function evaluations to a minimum.\nIn this synthetic example, we actually know the function, so we can just plot it for illustration purposes:\n# define a domain to evaluate line = np.linspace(0, 1) # evaluate function values = f(line) # plot function plt.plot(line, values) plt.xlabel(\u0026#34;Input (parameter)\u0026#34;) plt.ylabel(\u0026#34;Objective (i.e. performance)\u0026#34;) Text(0,0.5,'Objective (i.e. performance)')  Our goal here is to find the global minimum of this function, assuming that we don\u0026rsquo;t have direct access to the formula (given the formula, we could instead calculate the optimum quite precicely using methods from calculus instead). Usually, the function is too expensive to evaluate in such a manner, in particular in higher-dimensional spaces.\nNow, we use MLOS to construct an OptimizationProblem object that will encapsulate the function and the input space.\nfrom mlos.Optimizers.OptimizationProblem import OptimizationProblem, Objective from mlos.Optimizers.BayesianOptimizer import BayesianOptimizer from mlos.Spaces import SimpleHypergrid, ContinuousDimension # single continuous input dimension between 0 and 1 input_space = SimpleHypergrid(name=\u0026#34;input\u0026#34;, dimensions=[ContinuousDimension(name=\u0026#34;x\u0026#34;, min=0, max=1)]) # define output space, we might not know the exact ranges output_space = SimpleHypergrid(name=\u0026#34;objective\u0026#34;, dimensions=[ContinuousDimension(name=\u0026#34;function_value\u0026#34;, min=-10, max=10)]) # define optimization problem with input and output space and objective optimization_problem = OptimizationProblem( parameter_space=input_space, objective_space=output_space, # we want to minimize the function objectives=[Objective(name=\u0026#34;function_value\u0026#34;, minimize=True)] ) The way Bayesian Optimization (in particular what is known as sequential model-based optimization) works is by iterating the following steps:\n Evaluate the function at a candidate point x_i (start with a random point x_0), observe f(x_i). Build / update a surrogate model g_i of the objective function (here a Random Forest) using the pairs x_i, f(x_i) that we observed so far. Pick the next data point to evaluate based on the updated model g_i using a criterion known as acquisition function.  The idea is that eventually the surrogate model will provide a good approximation of the objective function, but it will be much faster to evaluate (i.e. by predicting with a Random Forest or Gaussian process or another trained machine learning model, instead of running a complex deployment). The acquisition function serves as a means to trade off exploration vs exploitation in collecting new data for building the surrogate model: it picks points that have a low (close to optimum) value of the surrogate model (and so are expected to have a low value of the actual objective). This is the \u0026ldquo;exploitation\u0026rdquo; of existing knowledge in the model. On the other hand, it also encourages exploring new areas in which there is a lot of uncertainty in the surrogate model, i.e. where we expect the surrogate model not to be very acurate yet.\nThis process is coordinated by the BayesianOptimizer object, which we will use to perform Bayesian Optimization with a random forest surrogate model. Details of this particular method can be found in Hutter et. al. (2011). We\u0026rsquo;re first configuring the model to refit after every iteration and use 10 trees for the random forest:\nfrom mlos.Optimizers.BayesianOptimizerConfigStore import bayesian_optimizer_config_store from mlos.Optimizers.BayesianOptimizerFactory import BayesianOptimizerFactory from mlos.Spaces import Point # configure the optimizer, start from the default configuration optimizer_config = bayesian_optimizer_config_store.default # set the fraction of randomly sampled configuration to 10% of suggestions optimizer_config.experiment_designer_config.fraction_random_suggestions = .1 # configure the random forest surrogate model random_forest_config = optimizer_config.homogeneous_random_forest_regression_model_config # refit the model after each observation random_forest_config.decision_tree_regression_model_config.n_new_samples_before_refit = 1 # Use the best split in trees (not random as in extremely randomized trees) random_forest_config.decision_tree_regression_model_config.splitter = \u0026#39;best\u0026#39; # right now we\u0026#39;re sampling without replacement so we need to subsample # to make the trees different when using the \u0026#39;best\u0026#39; splitter random_forest_config.samples_fraction_per_estimator = .9 # Use 10 trees in the random forest (usually more are better, 10 makes it run pretty quickly) random_forest_config.n_estimators = 10 # Set multiplier for the confidence bound optimizer_config.experiment_designer_config.confidence_bound_utility_function_config.alpha = 0.1 optimizer_factory = BayesianOptimizerFactory() optimizer = optimizer_factory.create_local_optimizer( optimization_problem=optimization_problem, optimizer_config=optimizer_config ) 09/26/2020 19:29:36 - BayesianOptimizerFactory - INFO - [BayesianOptimizerFactory.py: 41 - create_local_optimizer() ] Creating a bayesian optimizer with config: { \u0026quot;surrogate_model_implementation\u0026quot;: \u0026quot;HomogeneousRandomForestRegressionModel\u0026quot;, \u0026quot;experiment_designer_implementation\u0026quot;: \u0026quot;ExperimentDesigner\u0026quot;, \u0026quot;min_samples_required_for_guided_design_of_experiments\u0026quot;: 10, \u0026quot;homogeneous_random_forest_regression_model_config.n_estimators\u0026quot;: 10, \u0026quot;homogeneous_random_forest_regression_model_config.features_fraction_per_estimator\u0026quot;: 1, \u0026quot;homogeneous_random_forest_regression_model_config.samples_fraction_per_estimator\u0026quot;: 0.9, \u0026quot;homogeneous_random_forest_regression_model_config.regressor_implementation\u0026quot;: \u0026quot;DecisionTreeRegressionModel\u0026quot;, \u0026quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.criterion\u0026quot;: \u0026quot;mse\u0026quot;, \u0026quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.splitter\u0026quot;: \u0026quot;best\u0026quot;, \u0026quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_depth\u0026quot;: 0, \u0026quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_split\u0026quot;: 2, \u0026quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_leaf\u0026quot;: 3, \u0026quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_weight_fraction_leaf\u0026quot;: 0, \u0026quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_features\u0026quot;: \u0026quot;auto\u0026quot;, \u0026quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.max_leaf_nodes\u0026quot;: 0, \u0026quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_impurity_decrease\u0026quot;: 0, \u0026quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.ccp_alpha\u0026quot;: 0, \u0026quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.min_samples_to_fit\u0026quot;: 10, \u0026quot;homogeneous_random_forest_regression_model_config.decision_tree_regression_model_config.n_new_samples_before_refit\u0026quot;: 1, \u0026quot;homogeneous_random_forest_regression_model_config.bootstrap\u0026quot;: 1, \u0026quot;experiment_designer_config.utility_function_implementation\u0026quot;: \u0026quot;ConfidenceBoundUtilityFunction\u0026quot;, \u0026quot;experiment_designer_config.numeric_optimizer_implementation\u0026quot;: \u0026quot;RandomSearchOptimizer\u0026quot;, \u0026quot;experiment_designer_config.confidence_bound_utility_function_config.utility_function_name\u0026quot;: \u0026quot;upper_confidence_bound_on_improvement\u0026quot;, \u0026quot;experiment_designer_config.confidence_bound_utility_function_config.alpha\u0026quot;: 0.1, \u0026quot;experiment_designer_config.random_search_optimizer_config.num_samples_per_iteration\u0026quot;: 1000, \u0026quot;experiment_designer_config.fraction_random_suggestions\u0026quot;: 0.5, \u0026quot;experiment_designer_config_fraction_random_suggestions\u0026quot;: 0.1 }.  Now, we can run the actual optimization which will carry out the steps outlined above.\ndef run_optimization(optimizer): # suggest new value from optimizer suggested_value = optimizer.suggest() input_values_df = suggested_value.to_dataframe() # suggested value are dictionary-like, keys are input space parameter names # evaluate target function target_value = f(suggested_value[\u0026#39;x\u0026#39;]) print(suggested_value.to_json(), target_value) # build dataframes to  target_values_df = pd.DataFrame({\u0026#39;function_value\u0026#39;: [target_value]}) optimizer.register(input_values_df, target_values_df) # run for some iterations n_iterations = 15 for i in range(n_iterations): run_optimization(optimizer) {\u0026quot;x\u0026quot;: 0.983092543436823} 15.17416093448022 {\u0026quot;x\u0026quot;: 0.8435385550792688} -1.4996779115157042 {\u0026quot;x\u0026quot;: 0.23007923495667082} -0.36288334233535546 {\u0026quot;x\u0026quot;: 0.15719066911289126} -0.9563346460889276 {\u0026quot;x\u0026quot;: 0.520939819354775} 0.9848498586246127 {\u0026quot;x\u0026quot;: 0.19141441981931262} -0.7187454908362136 {\u0026quot;x\u0026quot;: 0.8582062972543654} 0.15163828039842148 {\u0026quot;x\u0026quot;: 0.15134098015340036} -0.9751882528295707 {\u0026quot;x\u0026quot;: 0.11781845586576145} -0.8816797092480978 {\u0026quot;x\u0026quot;: 0.5836141907587765} 0.31070790925443953 {\u0026quot;x\u0026quot;: 0.686066698158629} -3.97383368468424 {\u0026quot;x\u0026quot;: 0.6204630856295394} -0.888327556915524 {\u0026quot;x\u0026quot;: 0.5350401764159637} 0.9670120759968829 {\u0026quot;x\u0026quot;: 0.7463592300760595} -5.959462466805426 {\u0026quot;x\u0026quot;: 0.9057244430736491} 6.518041099261522  You can see that the hit rate is not always increasing, and that is for two reasons: first, the optimizer keeps exploring parts of the space in which there is uncertainty. Second, by default the BayesianOptimizer picks a fraction of points at random to increase exploration. This fraction is set as optimizer_config.experiment_designer_config_fraction_random_suggestions = .1 above.\nAfter 15 iterations, the model is likely to have captured the general shape, but probably not have found the actual optimum:\n# evaluate the surrogate # surrogate_predictions = optimizer.predict(pd.DataFrame({\u0026#39;x\u0026#39;: line})).get_dataframe() # plot observations # feature_values, target_values, _ = optimizer.get_all_observations() plt.scatter(feature_values, target_values, label=\u0026#39;observed points\u0026#39;) # plot true function (usually unknown) # plt.plot(line, values, label=\u0026#39;true function\u0026#39;) # plot the surrogate # alpha = optimizer_config.experiment_designer_config.confidence_bound_utility_function_config.alpha t_values = t.ppf(1 - alpha / 2.0, surrogate_predictions[\u0026#39;predicted_value_degrees_of_freedom\u0026#39;]) ci_radii = t_values * np.sqrt(surrogate_predictions[\u0026#39;predicted_value_variance\u0026#39;]) value = surrogate_predictions[\u0026#39;predicted_value\u0026#39;] plt.plot(line, value, label=\u0026#39;surrogate predictions g\u0026#39;) plt.fill_between(line, value - ci_radii, value + ci_radii, alpha=.1) plt.plot(line, -optimizer.experiment_designer.utility_function(optimization_problem.construct_feature_dataframe(pd.DataFrame({\u0026#39;x\u0026#39;: line}))), \u0026#39;:\u0026#39;, label=\u0026#39;utility_function\u0026#39;) plt.ylabel(\u0026#34;Objective function f (performance)\u0026#34;) plt.xlabel(\u0026#34;Input variable\u0026#34;) plt.legend() \u0026lt;matplotlib.legend.Legend at 0x1fbe2a58b00\u0026gt;  We can find the best value according to the current surrogate with the optimum method:\noptimizer.optimum() ({ \u0026quot;x\u0026quot;: 0.7463592300760595 }, { \u0026quot;function_value\u0026quot;: -5.959462466805426 })  We can run more iterations to improve the surrogate model and the optimum that is found:\n# run for more iterations n_iterations = 50 for i in range(n_iterations): run_optimization(optimizer) {\u0026quot;x\u0026quot;: 0.473412447846017} 0.7021164245077595 {\u0026quot;x\u0026quot;: 0.6632242079785616} -2.8567197230243733 {\u0026quot;x\u0026quot;: 0.18807256140072703} -0.7483728822439081 {\u0026quot;x\u0026quot;: 0.4172371207492612} 0.21419007910211812 {\u0026quot;x\u0026quot;: 0.4946267758016123} 0.8749554960705916 {\u0026quot;x\u0026quot;: 0.6175708715122464} -0.7737011075417488 {\u0026quot;x\u0026quot;: 0.7443496510664342} -5.935312134676282 {\u0026quot;x\u0026quot;: 0.7439459529541941} -5.930000321904706 {\u0026quot;x\u0026quot;: 0.3767786032488877} 0.033842030625167815 {\u0026quot;x\u0026quot;: 0.3298466838855343} -1.8305512156868478e-05 {\u0026quot;x\u0026quot;: 0.14169507729653608} -0.9862030605731403 {\u0026quot;x\u0026quot;: 0.2111860703491355} -0.5341585508413712 {\u0026quot;x\u0026quot;: 0.7596100164652652} -6.017742850545829 {\u0026quot;x\u0026quot;: 0.8006992516779171} -4.9126248393567495 {\u0026quot;x\u0026quot;: 0.8109406889057383} -4.305536957072894 {\u0026quot;x\u0026quot;: 0.8181655852320083} -3.7961625204914613 {\u0026quot;x\u0026quot;: 0.2566480065634811} -0.16845997730008264 {\u0026quot;x\u0026quot;: 0.22836320880807248} -0.37762613491139096 {\u0026quot;x\u0026quot;: 0.8234842634097982} -3.378993806987811 {\u0026quot;x\u0026quot;: 0.1136365546027639} -0.8410156758386963 {\u0026quot;x\u0026quot;: 0.8515058915961442} -0.6289596894583336 {\u0026quot;x\u0026quot;: 0.5864306840092383} 0.2403755540305389 {\u0026quot;x\u0026quot;: 0.6530665956022451} -2.3573665797713597 {\u0026quot;x\u0026quot;: 0.6289697629244638} -1.2427732828786415 {\u0026quot;x\u0026quot;: 0.8239417825768095} -3.341461632283429 {\u0026quot;x\u0026quot;: 0.7333477284034062} -5.738233990022671 {\u0026quot;x\u0026quot;: 0.8315754575407727} -2.6775285696863467 {\u0026quot;x\u0026quot;: 0.5222880051350126} 0.9858420597039397 {\u0026quot;x\u0026quot;: 0.11642041423252358} -0.8688859210021689 {\u0026quot;x\u0026quot;: 0.4472894351503083} 0.4578657933743529 {\u0026quot;x\u0026quot;: 0.8527127293048408} -0.49149171666093044 {\u0026quot;x\u0026quot;: 0.7779341551755734} -5.779966997170833 {\u0026quot;x\u0026quot;: 0.7452511869304634} -5.946620760879891 {\u0026quot;x\u0026quot;: 0.5559819176453921} 0.8079207413333107 {\u0026quot;x\u0026quot;: 0.7494998662235278} -5.9894045479395 {\u0026quot;x\u0026quot;: 0.7139893782357419} -5.161987725383618 {\u0026quot;x\u0026quot;: 0.846796666877744} -1.15152680830581 {\u0026quot;x\u0026quot;: 0.6879805421463292} -4.063990746002142 {\u0026quot;x\u0026quot;: 0.7905230980834056} -5.381732629200189 {\u0026quot;x\u0026quot;: 0.4816517251028316} 0.7747029205006717 {\u0026quot;x\u0026quot;: 0.692753881917271} -4.284669550477883 {\u0026quot;x\u0026quot;: 0.9929490981264024} 15.633823386883824 {\u0026quot;x\u0026quot;: 0.7489069999749162} -5.984492737405131 {\u0026quot;x\u0026quot;: 0.7379839661326224} -5.834196772620266 {\u0026quot;x\u0026quot;: 0.7201435179072647} -5.372952605375765 {\u0026quot;x\u0026quot;: 0.8297734821402678} -2.840599351113572 {\u0026quot;x\u0026quot;: 0.14807885573973778} -0.9818647703658573 {\u0026quot;x\u0026quot;: 0.32904609824262243} -3.4027030163747833e-05 {\u0026quot;x\u0026quot;: 0.009055711748202966} 2.579698434938776 {\u0026quot;x\u0026quot;: 0.6368149488510908} -1.590145461988316  There is some improvement in the optimum:\nbest_observation_config, best_observation_value = optimizer.optimum() print(best_observation_config.to_json()) print(best_observation_value.to_json()) {\u0026quot;x\u0026quot;: 0.7596100164652652} {\u0026quot;function_value\u0026quot;: -6.017742850545829}  We can now visualize the surrogate model and optimization process again. The points are colored according to the iteration number, with dark blue points being early in the process and yellow points being later. You can see that at the end of the optimization, the points start to cluster around the optimum.\n# evaluate the surrogate # surrogate_predictions = optimizer.predict(pd.DataFrame({\u0026#39;x\u0026#39;: line})).get_dataframe() # plot observations # feature_values, target_values = optimizer.get_all_observations() plt.scatter(feature_values, target_values, label=\u0026#39;observed points\u0026#39;) # plot true function (usually unknown) # plt.plot(line, values, label=\u0026#39;true function\u0026#39;) # plot the surrogate # alpha = optimizer_config.experiment_designer_config.confidence_bound_utility_function_config.alpha t_values = t.ppf(1 - alpha / 2.0, surrogate_predictions[\u0026#39;predicted_value_degrees_of_freedom\u0026#39;]) ci_radii = t_values * np.sqrt(surrogate_predictions[\u0026#39;predicted_value_variance\u0026#39;]) value = surrogate_predictions[\u0026#39;predicted_value\u0026#39;] plt.plot(line, value, label=\u0026#39;surrogate predictions g\u0026#39;) plt.fill_between(line, value - ci_radii, value + ci_radii, alpha=.1) plt.plot(line, -optimizer.experiment_designer.utility_function(pd.DataFrame({\u0026#39;x\u0026#39;: line})), \u0026#39;:\u0026#39;, label=\u0026#39;utility_function\u0026#39;) ax = plt.gca() ax.set_ylabel(\u0026#34;Objective function f\u0026#34;) ax.set_xlabel(\u0026#34;Input variable\u0026#34;) bins_axes = ax.twinx() bins_axes.set_ylabel(\u0026#34;Points sampled\u0026#34;) feature_values.hist(bins=20, ax=bins_axes, alpha=.3, color=\u0026#39;k\u0026#39;, label=\u0026#34;count of querry points\u0026#34;) plt.legend() \u0026lt;matplotlib.legend.Legend at 0x1fbe2a26828\u0026gt;  Going further:   Plot the optimum as a function of iterations. How long does it take for the optimization to converge? Is that stable over several random restarts?\n  Does changing the search to a purely random search (setting optimizer_config.experiment_designer_config_fraction_random_suggestions = 1) change how long the optimization takes to find the optimum?\n  Download BayesianOptimization.ipynb notebook "});index.add({'id':21,'href':'/MLOS/notebooks/LevelDbTuning/','title':"Level Db Tuning",'section':"Notebooks",'content':"Download LevelDbTuning.ipynb notebook LevelDB parameter tuning using MLOS What is Level DB LevelDB is a key value store built using Log Structured Merge Trees (LSMs) Wiki. LevelDB supports read, write, delete and range query (sorted iteration) operations.\nTypical to any database system, LevelDB also comes with a bunch of parameters which can be tuned according to the workload to get the best performance. Before going to the parameters, we\u0026rsquo;ll briefly describe the working of LevelDB. The source code, the architecture and a simple example of how to use LevelDB can be found here.\nLevelDB working LevelDB uses 7 levels to store the data, the amoung of data that can be stored in each of the levels after level 0 is $10^{level}$, so level 1 can store around 10 MB of data, level 2 around 100 MB and so on.\nAs shown the diagram above, the main components of LevelDB are the MemTable,the SSTable files and the log file. LeveDB is primarily optimized for writes.\nMemTable is an in memory data structure to which incoming writes are added after they are appended to the log file. MemTables are typically implemented using skip lists or B+ trees. The parameter write_buffer_size (paramter input at DB startup) can be used to control the size of the MemTable and the log file.\nOnce the MemTable reaches the write_buffer_size (Default 4MB), a new MemTable is created and the original MemTable is made immutable. This immutable MemTable is converted to a new SSTable in the background to be added to the Level 0 of the LSM tree.\nSSTable: It is a file in which the key value pairs are stored sorted by keys. The size of SSTable is controlled by the parameter called max_file_size (Default 2MB).\nOnce the number of SSTable at Level 0 reaches a certain threshold controlled by the paramter kL0_CompactionTrigger (Default 4), these files are merged with higher level overlapping files. If no files are present in the higher level, the files are combined using merge sort techniques and added to higher level. A new file is created for every 2 MB of data by default.\nFor higher levels from 1 to the maximum number of levels, compaction process (merging process) is triggered when the level gets filled.\nA detailed explanation of the working of LeveDB is presented here.\nLevelDB paramter tuning using MLOS In this lab we will be tuning some of the important start up time paramters of LevelDB and observe how it affects the performance. The parameters that we will be tuning are write_buffer_size and max_file_size to try to optimize the throughput and latency of LevelDB for Sequential and random workloads.\nLevelDB installation: Instruction on Ubuntu 18.04 Follow the commands below to get, compile and install LevelDB\nsudo apt update sudo apt-get install cmake git clone --recurse-submodules https://github.com/google/leveldb.git cd leveldb mkdir -p build \u0026amp;\u0026amp; cd build cmake -DCMAKE_BUILD_TYPE=Release .. \u0026amp;\u0026amp; cmake --build . Now, from the ~/leveldb/build directory, you should be able to execute ./db_bench, the microbenchmark which can be used to measure the performance of LevelDB for different workloads.\nPlease take a look at the db_bench.cc file in the ~/leveldb/benchmarks directory and get an idea about the input parameters and workloads that are possible.\nAn example command to run a workload that does random writes of 1M values with value size 100 B is:\n./db_bench --benchmarks=fillrandom --val_size=100 --num=1000000 The output of the command will look like (numbers might be different):\nLevelDB: version 1.22 Date: Thu Oct 8 13:56:00 2020 CPU: 40 * Intel(R) Xeon(R) CPU E5-2660 v3 @ 2.60GHz CPUCache: 25600 KB Keys: 16 bytes each Values: 100 bytes each (50 bytes after compression) Entries: 1000000 RawSize: 110.6 MB (estimated) FileSize: 62.9 MB (estimated) WARNING: Snappy compression is not enabled ------------------------------------------------ Opening the DB now In the collect stats thread Total data written = 421.9 MB fillrandom : 31.731 micros/op; 3.5 MB/s In the subsequent cells, we will be using the Bayesian Optimization Python libraries from the MLOS to tune some startup time parameters to obtain the values that result in best throughput and latency.\nYou will have to install the python MLOS library and switch to the corresponding conda environment before moving further. Follow the steps at https://microsoft.github.io/MLOS/documentation/01-Prerequisites/#python-quickstart for installing python MLOS library.\nimport subprocess import matplotlib.pyplot as plt import numpy as np import pandas as pd from scipy.stats import t from mlos.Optimizers.OptimizationProblem import OptimizationProblem, Objective from mlos.Optimizers.BayesianOptimizer import BayesianOptimizer from mlos.Spaces import SimpleHypergrid, ContinuousDimension, DiscreteDimension from mlos.Optimizers.BayesianOptimizerConfigStore import bayesian_optimizer_config_store from mlos.Optimizers.BayesianOptimizerFactory import BayesianOptimizerFactory from mlos.Spaces import Point from mlos.Logger import create_logger import logging # configure the optimizer, start from the default configuration optimizer_config = bayesian_optimizer_config_store.default # set the fraction of randomly sampled configuration to 10% of suggestions optimizer_config.experiment_designer_config_fraction_random_suggestions = .1 # configure the random forest surrogate model random_forest_config = optimizer_config.homogeneous_random_forest_regression_model_config # refit the model after each observation random_forest_config.decision_tree_regression_model_config.n_new_samples_before_refit = 1 # Use the best split in trees (not random as in extremely randomized trees) random_forest_config.decision_tree_regression_model_config.splitter = \u0026#39;best\u0026#39; # right now we\u0026#39;re sampling without replacement so we need to subsample # to make the trees different when using the \u0026#39;best\u0026#39; splitter random_forest_config.samples_fraction_per_estimator = .9 # Use 10 trees in the random forest (usually more are better, 10 makes it run pretty quickly) random_forest_config.n_estimators = 10 # Set multiplier for the confidence bound optimizer_config.experiment_designer_config.confidence_bound_utility_function_config.alpha = 0.1 # optimizer = optimizer_factory.create_local_optimizer( # optimization_problem=optimization_problem, # optimizer_config=optimizer_config # ) # You will have to change the min and max value based on the start up time parameter that you want explore, # you can change the function or create a similar function for the going further questions.  def create_parameter_search_space(parameter_name, min_val, max_val): parameter_search_space = SimpleHypergrid( name=\u0026#39;parameter_config\u0026#39;, dimensions=[ DiscreteDimension(name=parameter_name, min=min_val, max=max_val) ] ) return parameter_search_space # Optimization Problem # You will have to set the min and max value based on the objective that you are using  def create_optimization_problem(parameter_search_space, objective_name, min_val, max_val, minimize=False): optimization_problem = OptimizationProblem( parameter_space=parameter_search_space, objective_space=SimpleHypergrid(name=\u0026#34;objectives\u0026#34;, dimensions=[ContinuousDimension(name=objective_name, min=min_val, max=max_val)]), objectives=[Objective(name=objective_name, minimize=minimize)] ) return optimization_problem # The max value here (1000) is used as an example # Set the max and the min value appropriately based on the objective metric and the hardware type. logger = create_logger(\u0026#39;Optimizing LevelDB\u0026#39;, logging_level=logging.WARN) def initialize_optimizer(): parameter_search_space = create_parameter_search_space(parameter_name=\u0026#34;write_buffer_size\u0026#34;, min_val=1*1024*1024, max_val=128*1024*1024) optimization_problem = create_optimization_problem(parameter_search_space, objective_name=\u0026#34;throughput\u0026#34;, min_val=0, max_val=1000, minimize=False) optimizer_factory = BayesianOptimizerFactory(logger=logger) optimizer = optimizer_factory.create_local_optimizer( optimization_problem=optimization_problem, optimizer_config=optimizer_config) return optimizer # Please change the leveldb_path to the build directory of your leveldb installation leveldb_path = \u0026#34;$HOME/leveldb/build/\u0026#34; # You can change the command to run a different kind of workload (take a look at db_bench.cc to see the possible workloads) command = \u0026#34;db_bench\u0026#34; # You might have to change the run workload function to explore a combination of parameters simultaneously def run_workload(workload, input_parameter, parameter_value): # The line below executes the db_bench command with approprite parameters, you can change this  # if you want to specify other input parameters result = subprocess.check_output(leveldb_path + command + \u0026#34; --benchmarks=\u0026#34; + workload + \u0026#34; --\u0026#34; + str(input_parameter) + \u0026#34;=\u0026#34; + str(parameter_value), shell=True) stats = (str(result).split(\u0026#34;:\u0026#34;)[-1]).split(\u0026#34;;\u0026#34;) # The line below is used to parse the output that is returned by db_bench latency, throughput = float(stats[0].strip().split(\u0026#34; \u0026#34;)[0]), float(stats[1].strip().split(\u0026#34; \u0026#34;)[0]) return latency, throughput def run_optimizer(): optimizer = initialize_optimizer() for i in range(10): new_config_values = optimizer.suggest() new_parameter_value = new_config_values[\u0026#34;write_buffer_size\u0026#34;] latency, throughput = run_workload(\u0026#34;fillrandom\u0026#34;, \u0026#34;write_buffer_size\u0026#34;, new_parameter_value) print(\u0026#34;Parameter value: {0:.2f} MB, Objective value: {1:.2f} MB/s\u0026#34;.format( float(new_parameter_value)/(1024*1024), float(throughput))) if i \u0026gt; 0: optimum_parameter, optimum_value = optimizer.optimum() print(\u0026#34;Optimal parameter: {0:.2f} MB, Optimal value: {1:.2f} MB/s\u0026#34;.format( float(optimum_parameter[\u0026#34;write_buffer_size\u0026#34;])/(1024*1024), optimum_value[\u0026#34;throughput\u0026#34;])) objectives_df = pd.DataFrame({\u0026#39;throughput\u0026#39;: [throughput]}) features_df = new_config_values.to_dataframe() optimizer.register(features_df, objectives_df) # Remember to call initialize_optimizer function before the run_optimizer # To avoid the optimizer remembering the optimal values from previous run run_optimizer() Parameter value: 4.34 MB, Objective value: 13.20 MB/s Parameter value: 63.73 MB, Objective value: 33.00 MB/s Optimal parameter: 4.34 MB, Optimal value: 13.20 MB/s Parameter value: 110.00 MB, Objective value: 31.60 MB/s Optimal parameter: 63.73 MB, Optimal value: 33.00 MB/s Parameter value: 40.79 MB, Objective value: 33.80 MB/s Optimal parameter: 63.73 MB, Optimal value: 33.00 MB/s Parameter value: 91.23 MB, Objective value: 32.50 MB/s Optimal parameter: 40.79 MB, Optimal value: 33.80 MB/s Parameter value: 45.60 MB, Objective value: 29.00 MB/s Optimal parameter: 40.79 MB, Optimal value: 33.80 MB/s Parameter value: 75.85 MB, Objective value: 29.60 MB/s Optimal parameter: 40.79 MB, Optimal value: 33.80 MB/s Parameter value: 105.25 MB, Objective value: 31.80 MB/s Optimal parameter: 40.79 MB, Optimal value: 33.80 MB/s Parameter value: 100.94 MB, Objective value: 32.30 MB/s Optimal parameter: 40.79 MB, Optimal value: 33.80 MB/s Parameter value: 95.30 MB, Objective value: 32.50 MB/s Optimal parameter: 40.79 MB, Optimal value: 33.80 MB/s  Verification Manually run the benchmark for various values of the parameter that you are testing, plot the graphs and verify if the optimal returned by the optimizer matches with the one manually obtained. For example, if the input_parameter is write_buffer_size, you can start from 2 MB (2097152) and go up to 64 MB (67108864), by trying values like, 2MB, 4MB, 8MB, 16MB, 32MB, 64MB and verify the point of deflection i.e the point where throughput starts to decrease after increasing or latency starts to increase after decreasing and verify if it matches with what is returned by the optimizer.\nGoing further In the questions below, you can choose to optimize either for throughput or latency (choose one). Plot graphs indicating how many iterations does the optimizer take to converge. Report values of the optimal values of parameters obtained and the optimal value of the objective metric at these parameter values.\n  Choose 2 parameters from leveldb/include/leveldb/options.h file (this can include write_buffer_size and max_file_size) and try to tune them manually and using the optimizer and compare the results. Plot a graph of the optimal value vs the iteration.\nHint: These parameters can be passed in as startup time parameters, look at leveldb/benchmarks/db_bench.cc for the possible startup time parameters.\n  Try tuning the performance metric by using a combination of startup time parameters. For example, you can try to optimize a combination of write_buffer_size and max_file_size together to obtain the best throughput.\nHint: You will have to make changes to the parameter search space. You will have to add a second parameter, look at the SmartCache example to do this.\n  Apart from the startup time parameters, LevelDB has compile time parameters (in leveldb/db/dbformat.h), choose parameters from this file that you think would affect the throughput or latency, make them start up time parameters and tune them using MLOS (some candidates: kl0_compaction_trigger, kl0_slowdownwritetrigger, kl0_stopwritetrigger, etc.). Look at the places where these parameters are used and what effect they can have on the performance.\nHint: To make these parameters startup time, look at the Open() function in leveldb/benchmarks/db_bench.cc and note how startup time parameters can be passed using Options structure.\n  Reference   https://wiesen.github.io/post/leveldb-storage-memtable/\n  https://www.igvita.com/2012/02/06/sstable-and-log-structured-storage-leveldb/\nDownload LevelDbTuning.ipynb notebook   "});index.add({'id':22,'href':'/MLOS/notebooks/SmartCacheCPP/','title':"Smart Cache C P P",'section':"Notebooks",'content':"Download SmartCacheCPP.ipynb notebook Connecting MLOS to a C++ application This notebook walks through connecting MLOS to a C++ application within a docker container. We will start a docker container, and run an MLOS Agent within it. The MLOS Agent will start the actual application, and communicate with it via a shared memory channel. In this example, the MLOS Agent controls the execution of the workloads on the application, and we will later connect to the agent to optimize the configuration of our application.\nThe application is a \u0026ldquo;SmartCache\u0026rdquo; similar to the one in the SmartCacheOptimization notebook, though with some more parameters to tune. The source for this example is in the source/Examples/SmartCache folder.\nBuilding the application To build and run the necessary components for this example you need to create and run a docker image. To that end, open a separate terminal and go to the MLOS main folder. Within that folder, run the following commands:\n  Build the Docker image using the Dockerfile at the root of the repository.\ndocker build --build-arg=UbuntuVersion=20.04 -t mlos-build-ubuntu-20.04 .   Run the Docker image you just built.\ndocker run -it -v $PWD:/src/MLOS -p 127.0.0.1:50051:50051/tcp \\  --name mlos-build mlos-build-ubuntu-20.04 This will open a shell inside the docker container. We\u0026rsquo;re also exposing port 50051 on the docker container to port 50051 of our host machine. This will allow us later to connect to the optimizer that runs inside the docker container.\n  Inside the container, build the compiled software with make:\nmake dotnet-build cmake-build cmake-install   The relevant output will be at:\n  Mlos.Agent.Server:\nThis file corresponds to the main entry point for MLOS, written in C#. You can find the source in source/Mlos.Agent.Server/MlosAgentServer.cs and the binary at target/bin/Release/AnyCPU/Mlos.Agent.Server/Mlos.Agent.Server.dll\n  SmartCache:\nThis is the C++ executable that implements the SmartCache and executes some workloads. You can find the source in source/Examples/SmartCache/Main.cpp and the binary at target/bin/Release/x86_64/SmartCache\n  SmartCache.SettingsRegistry:\nThis is the C# code that declares the configuration options for the SmartCache component, and defines the communication between the the MLOS Agent and the SmartCache component. You can find the source in source/Examples/SmartCache/SmartCache.SettingsRegistry/AssemblyInitializer.cs and the binary at target/bin/Release/AnyCPU/SmartCache.SettingsRegistry.dll\n  SmartCache.ExperimentSession:\nThis is the C# code that helps coordinate between the SmartCache component and the Mlos.Agent using the messages defined in the SmartCache.SettingsRegistry. Its main purpose is to process telemetry messages (e.g. aggregate), relay requests to the optimizer over Grpc (setting up a search space for a new optimizer if necessary or referencing an existing one), and feed the suggestions back to the SmartCache component for reconfiguration.\n  Starting the MLOS Agent and executing the workloads: Within the docker container, we can now tell the agent where the configuration options are stored, by setting the --settings-registry-path for the MLOS Agent which will also run the SmartCache executable.\ndotnet target/bin/Release/AnyCPU/Mlos.Agent.Server/Mlos.Agent.Server.dll \\  --settings-registry-path target/bin/Release/AnyCPU \\  --executable target/bin/Release/x86_64/SmartCache \\  --experiment ./target/bin/Release/AnyCPU/SmartCache.ExperimentSession/SmartCache.ExperimentSession.dll For a more detailed explanation of what\u0026rsquo;s going on, please see the SmartCache README.\nThe main loop of SmartCache contains the following:\nfor (int observations = 0; observations \u0026lt; 100; observations++) { // run 100 observations  std::cout \u0026lt;\u0026lt; \u0026#34;observations: \u0026#34; \u0026lt;\u0026lt; observations \u0026lt;\u0026lt; std::endl; for (int i = 0; i \u0026lt; 20; i++) { // run a workload 20 times  CyclicalWorkload(2048, smartCache); } bool isConfigReady = false; std::mutex waitForConfigMutex; std::condition_variable waitForConfigCondVar; // Setup a callback.  //  // OMMITTED  // [...]  // Send a request to obtain a new configuration.  SmartCache::RequestNewConfigurationMessage msg = { 0 }; mlosContext.SendTelemetryMessage(msg); // wait for MLOS Agent so send a message with a new configuration  std::unique_lock\u0026lt;std::mutex\u0026gt; lock(waitForConfigMutex); while (!isConfigReady) { waitForConfigCondVar.wait(lock); } config.Update(); smartCache.Reconfigure(); } After each iteration, a TelemetryMessage is sent to the MLOS Agent, and the SmartCache blocks until it receives a new configuration to run the next workload. By default, the agent is not connected to any optimizer, and will not change the original configuration, so the workload will just run uninterrupted.\nStarting an Optimizer We can now also start an Optimizer service for the MLOS Agent to connect to so that we can actually optimize the parameters for this workload. As the optimizer is running in a separate process, we need to create a new shell on the running docker container using the following command:\ndocker exec -it mlos-build /bin/bash Within the container, we now install the Python optimizer service:\npip install -e source/Mlos.Python/ And run it:\nstart_optimizer_microservice launch --port 50051 Connecting the Agent to the Optimizer Now we can start the agent again, this time also pointing it to the optimizer:\ndotnet target/bin/Release/AnyCPU/Mlos.Agent.Server/Mlos.Agent.Server.dll \\  --settings-registry-path ./target/bin/Release/AnyCPU/ \\  --executable target/bin/Release/x86_64/SmartCache \\  --experiment ./target/bin/Release/AnyCPU/SmartCache.ExperimentSession/SmartCache.ExperimentSession.dll \\  --optimizer-uri http://localhost:50051 This will run the workload again, this time using the optimizer to suggest better configurations. You should see output both in the terminal the agent is running in and in the terminal the OptimizerMicroservice is running in.\nInspecting results After (or even while) the optimization is running, we can connect to the optimizer via another GRPC channel. The optimizer is running within the docker container, but when we started docker, we exposed the port 50051 as the same port 50051 on the host machine (on which this notebook is running). So we can now connect to the optimizer within the docker container at 127.0.0.1:50051. This assumes this notebook runs in an environment with the mlos Python package installed (see the documentation).\nfrom mlos.Grpc.OptimizerMonitor import OptimizerMonitor import grpc # create a grpc channel and instantiate the OptimizerMonitor channel = grpc.insecure_channel(\u0026#39;127.0.0.1:50051\u0026#39;) optimizer_monitor = OptimizerMonitor(grpc_channel=channel) optimizer_monitor OptimizerMonitor(grpc_channel='127.0.0.1:50051')  # There should be one optimizer running in the docker container # corresponding to the C++ SmartCache optimization problem # An OptimizerMicroservice can run multiple optimizers, which would all be listed here optimizers = optimizer_monitor.get_existing_optimizers() optimizers [\u0026lt;mlos.Grpc.BayesianOptimizerProxy.BayesianOptimizerProxy at 0x7fdff5578d90\u0026gt;]  We can now get the observations exactly the same way as for the Python example in SmartCacheOptimization.ipynb\noptimizer = optimizers[0] features_df, objectives_df = optimizer.get_all_observations() import pandas as pd features, targets = optimizer.get_all_observations() data = pd.concat([features, targets], axis=1) data  cache_implementation lru_cache_config.cache_size \\ 0 LeastRecentlyUsed 100.0 1 MostRecentlyUsed NaN 2 MostRecentlyUsed NaN 3 LeastRecentlyUsed 3145.0 4 LeastRecentlyUsed 2927.0 .. ... ... 95 LeastRecentlyUsed 3726.0 96 LeastRecentlyUsed 3358.0 97 MostRecentlyUsed NaN 98 LeastRecentlyUsed 2081.0 99 MostRecentlyUsed NaN mru_cache_config.cache_size HitRate 0 NaN 0.000000 1 2352.0 0.950000 2 1931.0 0.896165 3 NaN 0.950000 4 NaN 0.950000 .. ... ... 95 NaN 0.950000 96 NaN 0.950000 97 3321.0 0.950000 98 NaN 0.950000 99 372.0 0.172643 [100 rows x 4 columns]  lru_data, mru_data = data.groupby(\u0026#39;cache_implementation\u0026#39;) import matplotlib.pyplot as plt line_lru = lru_data[1].plot(x=\u0026#39;lru_cache_config.cache_size\u0026#39;, y=\u0026#39;HitRate\u0026#39;, label=\u0026#39;LRU\u0026#39;, marker=\u0026#39;o\u0026#39;, linestyle=\u0026#39;none\u0026#39;, alpha=.6) mru_data[1].plot(x=\u0026#39;mru_cache_config.cache_size\u0026#39;, y=\u0026#39;HitRate\u0026#39;, label=\u0026#39;MRU\u0026#39;, marker=\u0026#39;o\u0026#39;, linestyle=\u0026#39;none\u0026#39;, alpha=.6, ax=plt.gca()) plt.ylabel(\u0026#34;Cache hitrate\u0026#34;) plt.xlabel(\u0026#34;Cache Size\u0026#34;) plt.legend() \u0026lt;matplotlib.legend.Legend at 0x7fdff453aad0\u0026gt;  Going Further  Instead of cache hit rate, use a metric based on runtime (e.g. latency, throughput, etc) as performance metric. Environment (context) sensitive metrics can also be measured (e.g. time. How does the signal from the runtime based metric compare to the application specific one (hit rate)? How consistent are the runtime results across multiple runs? Pick another widely used cache replacement policy such as LFU and construct a synthetic workload on which you expect this strategy to work well. Implement the policy and workload as part of the SmartCache example, and add a new option to the SmartCache.SettingsRegistry\\AssemblyInitializer.cs. Run the optimization again with your new workload. Does the optimizer find that your new policy performs best?  Download SmartCacheCPP.ipynb notebook "});index.add({'id':23,'href':'/MLOS/notebooks/SmartCacheOptimization/','title':"Smart Cache Optimization",'section':"Notebooks",'content':"Download SmartCacheOptimization.ipynb notebook Optimizing Smart Cache with Bayesian Optimization The goal of this notebook is to optimize SmartCache using Bayesian Optimization approach.\nWe\u0026rsquo;re using a sequential model-based optimization approach, that consists of the following loop:\n Get suggested config from optimizer, Apply suggested config to SmartCache, Execute a fixed workload, Collect the metrics from SmartCache, Register an observation with the optimizer.  # import the required classes and tools import grpc import pandas as pd import logging from mlos.Logger import create_logger from mlos.Examples.SmartCache import HitRateMonitor, SmartCache, SmartCacheWorkloadGenerator, SmartCacheWorkloadLauncher from mlos.Mlos.SDK import MlosExperiment from mlos.Optimizers.BayesianOptimizerFactory import BayesianOptimizerFactory from mlos.Optimizers.OptimizationProblem import OptimizationProblem, Objective from mlos.Spaces import Point, SimpleHypergrid, ContinuousDimension # The optimizer will be in a remote process via grpc, we pick the port here: grpc_port = 50051 Launch the optimizer service in a different process:\nimport subprocess optimizer_microservice = subprocess.Popen(f\u0026#34;start_optimizer_microservice launch --port {grpc_port}\u0026#34;, shell=True) Now the optimizer service that runs the surrogate model and suggests new points is started in the background. Next, we instantiate an object that connects to it over grpc using the BayesianOptimizerFactory.\nlogger = create_logger(\u0026#39;Optimizing Smart Cache\u0026#39;, logging_level=logging.WARN) optimizer_service_grpc_channel = grpc.insecure_channel(f\u0026#39;localhost:{grpc_port}\u0026#39;) bayesian_optimizer_factory = BayesianOptimizerFactory(grpc_channel=optimizer_service_grpc_channel, logger=logger) The optimization problem Then we can instantiate our optimization problem. We want to optimize the configuration of the SmartCache component that contains two implementations: an LRU (least recently used) cache and an MRU cache (most recently used). The SmartCache component has two parameters that we can adjust, the type of cache and the cache size. We are using some synthetic workloads for the cache and try to find what the optimum configuration for each workload is.\nHere, we measure \u0026lsquo;optimum\u0026rsquo; by the number of cache hits. Another option would be to measure runtime; however, this is a toy example with a trivial workload and there is likely substantial runtime difference. The parameter search space is declared in SmartCache.parameter_search_space:\nSmartCache.parameter_search_space  Name: smart_cache_config Dimensions: implementation: {LRU, MRU} IF implementation IN {LRU} THEN ( Name: lru_cache_config Dimensions: cache_size: {1, 2, ... , 4096} ) IF implementation IN {MRU} THEN ( Name: mru_cache_config Dimensions: cache_size: {1, 2, ... , 4096} )  The optimization problem is constructed using this parameter space as the input to optimize, and defines a single continuous objective, \u0026lsquo;hit_rate\u0026rsquo; between 0 and 1.\n# Optimization Problem # optimization_problem = OptimizationProblem( parameter_space=SmartCache.parameter_search_space, objective_space=SimpleHypergrid(name=\u0026#34;objectives\u0026#34;, dimensions=[ContinuousDimension(name=\u0026#34;hit_rate\u0026#34;, min=0, max=1)]), objectives=[Objective(name=\u0026#34;hit_rate\u0026#34;, minimize=False)] ) # create an optimizer proxy that connects to the remote optimizer via grpc: # here we could also configure the optimizer optimizer = bayesian_optimizer_factory.create_remote_optimizer(optimization_problem=optimization_problem) Defining workloads Now we can instantiate our workloads and stand up the MLOS infrastructure, both of which are orchestrated bySmartCacheWorkloadLauncher. The MLOS infrastructure consists of the MlosAgent and a communication channel, which are available to both the SmartCacheWorkloadGenerator and the SmartCache. The SmartCacheWorkloadLauncher launches workloads in SmartCacheWorkloadGenerator in a separate thread, which will actually generate and run the workloads for the smart cache. The SmartCacheWorkloadLauncher also connects the SmartCacheWorkLloadGenerator to the optimization problem via a MlosAgent that will consume the configurations.\nworkload_launcher = SmartCacheWorkloadLauncher(logger=logger) mlos_agent = workload_launcher.mlos_agent We set up the agent to consume configurations for the SmartCacheWorkloadGenerator, and we configure the workload to be sequential keys from a range from 0 to 2048.\nmlos_agent.set_configuration( component_type=SmartCacheWorkloadGenerator, new_config_values=Point( workload_type=\u0026#39;cyclical_key_from_range\u0026#39;, cyclical_key_from_range_config=Point( min=0, range_width=2048 ) ) ) Launching the experiment (measurement) Now we build the experiment, which collects hit-rate statistics from the SmartCacheWorkloadGenerator via the HitRateMonitor. This architecture reflects the native architecture for the C++ interface in which communication is done via shared memory between MLOS and the worker.\nhit_rate_monitor = HitRateMonitor() smart_cache_experiment = MlosExperiment( smart_component_types=[SmartCache], telemetry_aggregators=[hit_rate_monitor] ) mlos_agent.start_experiment(smart_cache_experiment) Performing the optimization Now that we have all the pieces in place, we can iterate our main optimization loop. Our workload will run in the same process as this notebook, but in a separate thread, which we block on. In a real example, the workload might run completely independent of our optimization procedure.\nWe run the optimization for 20 iterations, in each of which we obtain a new configuration from the optimizer (that interfaces the remote optimizer service). The configuration is passed to SmartCacheWorkloadGenerator via the MlosAgent, after which we start a blocking workload for 0.2 seconds. Then, the hit-rate (our objective) is read from the HitRateMonitor and the suggested configuration together with the resulting hit-rate are passed to the optimizer.\nnum_iterations = 100 data = [] for i in range(num_iterations): # suggest runs a \u0026#39;cheap\u0026#39; search on the surrogate model to find a good candidate configuration new_config_values = optimizer.suggest() # set_configuration communicates the proposed configuration to the SmartCache mlos_agent.set_configuration(component_type=SmartCache, new_config_values=new_config_values) hit_rate_monitor.reset() # start_workload will actually run the worker, here for 0.2 seconds workload_launcher.start_workload(duration_s=0.2, block=True) # obtain hit-rate as quality measure for configuration hit_rate = hit_rate_monitor.get_hit_rate() objectives_df = pd.DataFrame({\u0026#39;hit_rate\u0026#39;: [hit_rate]}) # pass configuration and observed hit-rate to the optimizer to update the surrogate model features_df = new_config_values.to_dataframe() optimizer.register(features_df, objectives_df) print(f\u0026#34;[{i+1}/{num_iterations}] current_config: {new_config_values.to_json()}, hit_rate: {hit_rate:.3f}\u0026#34;) [1/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 265}, hit_rate: 0.000 [2/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 2013}, hit_rate: 0.000 [3/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 630}, hit_rate: 0.000 [4/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;MRU\u0026quot;, \u0026quot;mru_cache_config.cache_size\u0026quot;: 1884}, hit_rate: 0.862 [5/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 2041}, hit_rate: 0.000 [6/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 3969}, hit_rate: 0.951 [7/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;MRU\u0026quot;, \u0026quot;mru_cache_config.cache_size\u0026quot;: 286}, hit_rate: 0.127 [8/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;MRU\u0026quot;, \u0026quot;mru_cache_config.cache_size\u0026quot;: 3323}, hit_rate: 0.953 [9/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;MRU\u0026quot;, \u0026quot;mru_cache_config.cache_size\u0026quot;: 1656}, hit_rate: 0.760 [10/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 198}, hit_rate: 0.000 [11/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;MRU\u0026quot;, \u0026quot;mru_cache_config.cache_size\u0026quot;: 3591}, hit_rate: 0.939 [12/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;MRU\u0026quot;, \u0026quot;mru_cache_config.cache_size\u0026quot;: 1854}, hit_rate: 0.855 [13/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 1381}, hit_rate: 0.000 [14/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 528}, hit_rate: 0.000 [15/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;MRU\u0026quot;, \u0026quot;mru_cache_config.cache_size\u0026quot;: 4023}, hit_rate: 0.952 [16/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 2829}, hit_rate: 0.958 [17/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 3562}, hit_rate: 0.950 [18/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;MRU\u0026quot;, \u0026quot;mru_cache_config.cache_size\u0026quot;: 2021}, hit_rate: 0.946 [19/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;MRU\u0026quot;, \u0026quot;mru_cache_config.cache_size\u0026quot;: 3892}, hit_rate: 0.938 [20/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;MRU\u0026quot;, \u0026quot;mru_cache_config.cache_size\u0026quot;: 1352}, hit_rate: 0.611 [21/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;MRU\u0026quot;, \u0026quot;mru_cache_config.cache_size\u0026quot;: 1596}, hit_rate: 0.729 [22/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;MRU\u0026quot;, \u0026quot;mru_cache_config.cache_size\u0026quot;: 1635}, hit_rate: 0.713 [23/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;MRU\u0026quot;, \u0026quot;mru_cache_config.cache_size\u0026quot;: 1671}, hit_rate: 0.766 [24/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;MRU\u0026quot;, \u0026quot;mru_cache_config.cache_size\u0026quot;: 3762}, hit_rate: 0.954 [25/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;MRU\u0026quot;, \u0026quot;mru_cache_config.cache_size\u0026quot;: 10}, hit_rate: 0.004 [26/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;MRU\u0026quot;, \u0026quot;mru_cache_config.cache_size\u0026quot;: 2022}, hit_rate: 0.943 [27/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 382}, hit_rate: 0.000 [28/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 2908}, hit_rate: 0.949 [29/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 2932}, hit_rate: 0.933 [30/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 2949}, hit_rate: 0.950 [31/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 2938}, hit_rate: 0.934 [32/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 2873}, hit_rate: 0.949 [33/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 2896}, hit_rate: 0.958 [34/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 2996}, hit_rate: 0.953 [35/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 1945}, hit_rate: 0.000 [36/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 2260}, hit_rate: 0.949 [37/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;MRU\u0026quot;, \u0026quot;mru_cache_config.cache_size\u0026quot;: 2872}, hit_rate: 0.956 [38/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 2917}, hit_rate: 0.950 [39/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 1032}, hit_rate: 0.000 [40/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 2351}, hit_rate: 0.957 [41/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 3178}, hit_rate: 0.953 [42/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 2407}, hit_rate: 0.959 [43/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;MRU\u0026quot;, \u0026quot;mru_cache_config.cache_size\u0026quot;: 3321}, hit_rate: 0.951 [44/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 2686}, hit_rate: 0.953 [45/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;MRU\u0026quot;, \u0026quot;mru_cache_config.cache_size\u0026quot;: 319}, hit_rate: 0.139 [46/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 2323}, hit_rate: 0.957 [47/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 2263}, hit_rate: 0.951 [48/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;MRU\u0026quot;, \u0026quot;mru_cache_config.cache_size\u0026quot;: 1596}, hit_rate: 0.729 [49/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;MRU\u0026quot;, \u0026quot;mru_cache_config.cache_size\u0026quot;: 3892}, hit_rate: 0.951 [50/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 2168}, hit_rate: 0.955 [51/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 2351}, hit_rate: 0.951 [52/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 2248}, hit_rate: 0.958 [53/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 2099}, hit_rate: 0.954 [54/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;MRU\u0026quot;, \u0026quot;mru_cache_config.cache_size\u0026quot;: 393}, hit_rate: 0.169 [55/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;MRU\u0026quot;, \u0026quot;mru_cache_config.cache_size\u0026quot;: 2391}, hit_rate: 0.952 [56/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 2102}, hit_rate: 0.959 [57/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 3240}, hit_rate: 0.951 [58/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 3361}, hit_rate: 0.956 [59/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;MRU\u0026quot;, \u0026quot;mru_cache_config.cache_size\u0026quot;: 1135}, hit_rate: 0.518 [60/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 2096}, hit_rate: 0.952 [61/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 2102}, hit_rate: 0.950 [62/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 2100}, hit_rate: 0.958 [63/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 2099}, hit_rate: 0.919 [64/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;MRU\u0026quot;, \u0026quot;mru_cache_config.cache_size\u0026quot;: 1404}, hit_rate: 0.608 [65/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 2100}, hit_rate: 0.954 [66/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 2091}, hit_rate: 0.949 [67/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;MRU\u0026quot;, \u0026quot;mru_cache_config.cache_size\u0026quot;: 3551}, hit_rate: 0.958 [68/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 2060}, hit_rate: 0.950 [69/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 1553}, hit_rate: 0.000 [70/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 2267}, hit_rate: 0.954 [71/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 2068}, hit_rate: 0.954 [72/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 1752}, hit_rate: 0.000 [73/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;MRU\u0026quot;, \u0026quot;mru_cache_config.cache_size\u0026quot;: 194}, hit_rate: 0.087 [74/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;MRU\u0026quot;, \u0026quot;mru_cache_config.cache_size\u0026quot;: 949}, hit_rate: 0.439 [75/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;MRU\u0026quot;, \u0026quot;mru_cache_config.cache_size\u0026quot;: 2010}, hit_rate: 0.930 [76/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 2073}, hit_rate: 0.958 [77/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 2058}, hit_rate: 0.948 [78/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 2068}, hit_rate: 0.951 [79/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 2058}, hit_rate: 0.958 [80/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 2062}, hit_rate: 0.952 [81/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;MRU\u0026quot;, \u0026quot;mru_cache_config.cache_size\u0026quot;: 234}, hit_rate: 0.103 [82/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 2044}, hit_rate: 0.000 [83/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 428}, hit_rate: 0.000 [84/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;MRU\u0026quot;, \u0026quot;mru_cache_config.cache_size\u0026quot;: 546}, hit_rate: 0.250 [85/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 1954}, hit_rate: 0.000 [86/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;MRU\u0026quot;, \u0026quot;mru_cache_config.cache_size\u0026quot;: 2741}, hit_rate: 0.949 [87/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 2026}, hit_rate: 0.000 [88/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;MRU\u0026quot;, \u0026quot;mru_cache_config.cache_size\u0026quot;: 3208}, hit_rate: 0.958 [89/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;MRU\u0026quot;, \u0026quot;mru_cache_config.cache_size\u0026quot;: 698}, hit_rate: 0.312 [90/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 2049}, hit_rate: 0.957 [91/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 2026}, hit_rate: 0.000 [92/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 2049}, hit_rate: 0.952 [93/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 2041}, hit_rate: 0.000 [94/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;MRU\u0026quot;, \u0026quot;mru_cache_config.cache_size\u0026quot;: 2773}, hit_rate: 0.956 [95/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 2043}, hit_rate: 0.000 [96/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 1976}, hit_rate: 0.000 [97/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;LRU\u0026quot;, \u0026quot;lru_cache_config.cache_size\u0026quot;: 1425}, hit_rate: 0.000 [98/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;MRU\u0026quot;, \u0026quot;mru_cache_config.cache_size\u0026quot;: 1024}, hit_rate: 0.465 [99/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;MRU\u0026quot;, \u0026quot;mru_cache_config.cache_size\u0026quot;: 2903}, hit_rate: 0.957 [100/100] current_config: {\u0026quot;implementation\u0026quot;: \u0026quot;MRU\u0026quot;, \u0026quot;mru_cache_config.cache_size\u0026quot;: 205}, hit_rate: 0.089  Analyzing results For a cyclical workload with 2048 keys, we assume that a MRU cache with a size of at least 2048 will perform best, and get 100% hits once the cache is filled. Now lets see the suggestions and results from the current experiment.\n# some pandas wrangling features, targets = optimizer.get_all_observations() data = pd.concat([features, targets], axis=1) data  implementation lru_cache_config.cache_size mru_cache_config.cache_size \\ 0 LRU 265.0 NaN 1 LRU 2013.0 NaN 2 LRU 630.0 NaN 3 MRU NaN 1884.0 4 LRU 2041.0 NaN .. ... ... ... 95 LRU 1976.0 NaN 96 LRU 1425.0 NaN 97 MRU NaN 1024.0 98 MRU NaN 2903.0 99 MRU NaN 205.0 hit_rate 0 2.911210e-10 1 4.093330e-10 2 4.675080e-10 3 8.619243e-01 4 3.941660e-10 .. ... 95 4.048580e-10 96 4.732610e-10 97 4.645777e-01 98 9.574132e-01 99 8.882438e-02 [100 rows x 4 columns]  # group by implementation, then plot lru_data, mru_data = data.groupby(\u0026#39;implementation\u0026#39;) import matplotlib.pyplot as plt line_lru = lru_data[1].plot(x=\u0026#39;lru_cache_config.cache_size\u0026#39;, y=\u0026#39;hit_rate\u0026#39;, label=\u0026#39;LRU\u0026#39;, marker=\u0026#39;o\u0026#39;, linestyle=\u0026#39;none\u0026#39;, alpha=.6) mru_data[1].plot(x=\u0026#39;mru_cache_config.cache_size\u0026#39;, y=\u0026#39;hit_rate\u0026#39;, label=\u0026#39;MRU\u0026#39;, marker=\u0026#39;o\u0026#39;, linestyle=\u0026#39;none\u0026#39;, alpha=.6, ax=plt.gca()) plt.ylabel(\u0026#34;Cache hitrate\u0026#34;) plt.xlabel(\u0026#34;Cache Size\u0026#34;) plt.legend() \u0026lt;matplotlib.legend.Legend at 0x7f24bb8a23d0\u0026gt;  We can see that if the cache size is over 2048 keys, it means everything can fit into the cache and the strategy does not matter. However, for smaller cache sizes, the MRU strategy has an obvious advantage over the LRU strategy.\nGoing Further   Log how the optimum evolves over time. How many iterations are needed?\n  Can you adjust options in the Optimizer to improve convergence (see the BayesianOptimization notebook for suggestions).\n  Choose a different workload in the SmartCacheWorkloadGenerator. How do the workloads change the optimum strategy?\n  Clean up We need to stop all processes \u0026amp; separate threads after running the experiments:\n# Clean up # mlos_agent.stop_experiment(smart_cache_experiment) mlos_agent.stop_all() # Stop the optimizer service import signal optimizer_microservice.send_signal(signal.SIGTERM) Download SmartCacheOptimization.ipynb notebook "});index.add({'id':25,'href':'/MLOS/source/Examples/','title':"Examples",'section':"Sources",'content':"Examples  Smart Cache  "});index.add({'id':26,'href':'/MLOS/source/Examples/SmartCache/','title':"Smart Cache",'section':"Examples",'content':"SmartCache Example This SmartCache example is a C++ implementation of the Python SmartCache.\nIt implements a simple cache with different replacement policies and cache size as built-in tunables and some simple workloads.\nIt can be used to demonstrate a full end-to-end MLOS integrated microbenchmark for a \u0026ldquo;smart\u0026rdquo; component (in this case a cache).\nContents  SmartCache Example  Contents Building  Linux Windows   Executing  Without an optimizer  Linux Windows Example output   With an optimizer  Example output     Explanation See Also    Building See Also\n General build instructions   Note: these commands are given relative to the root of the MLOS repo.\n Linux  You can pull or build the Docker image using the Dockerfile at the root of the repository to get a Linux build environment.\n make -C source/Mlos.Agent.Server make -C source/Examples/SmartCache all install  The install target places the output in the more convenient target/bin/... path.\n Windows msbuild /m /r source/Mlos.Agent.Server/Mlos.Agent.Server.csproj msbuild /m /r source/Examples/SmartCache/SmartCache.vcxproj Executing Without an optimizer Linux dotnet target/bin/Release/Mlos.Agent.Server.dll \\  --settings-registry-path target/bin/Release/AnyCPU \\  --experiment target/bin/Release/AnyCPU/SmartCache.ExperimentSession/SmartCache.ExperimentSession.dll \\  --executable target/bin/Release/x86_64/SmartCache Windows dotnet target/bin/Release/Mlos.Agent.Server.dll \\ --settings-registry-path target/bin/Release/AnyCPU \\ --experiment target/bin/Release/AnyCPU/SmartCache.ExperimentSession/SmartCache.ExperimentSession.dll \\ --executable target/bin/Release/x64/SmartCache.exe Example output Mlos.Agent.Server Starting target/bin/Release/x86_64/SmartCache observations: 0 info: Microsoft.Hosting.Lifetime[0] Now listening on: http://localhost:5000 info: Microsoft.Hosting.Lifetime[0] Application started. Press Ctrl+C to shut down. info: Microsoft.Hosting.Lifetime[0] Hosting environment: Production info: Microsoft.Hosting.Lifetime[0] Content root path: /src/MLOS Starting Mlos.Agent Found settings registry assembly at target/bin/Release/AnyCPU/SmartCache.SettingsRegistry.dll observations: 1 observations: 2 observations: 3 ... With an optimizer The examples above merely communicate the SmartCache\u0026rsquo;s progress with the external agent using shared memory.\nTo also have the SmartCache tune itself by connecting to an optimizer we need to\n  Start an optimizer:\nstart_optimizer_microservice launch --port 50051  This assumes that the mlos module has already been installed and is available on the command search PATH environment variable.\nSee the Python Quickstart documentation for details.\n   Add --optimizer-uri http://localhost:50051 to the set of arguments to the command above.\n  Example output dotnet target/bin/Release/AnyCPU/Mlos.Agent.Server/Mlos.Agent.Server.dll \\  --executable target/bin/Release/x86_64/SmartCache \\  --settings-registry-path target/bin/Release/AnyCPU \\  --experiment target/bin/Release/AnyCPU/SmartCache.ExperimentSession/SmartCache.ExperimentSession.dll \\  --optimizer-uri http://localhost:50051 ```txt Mlos.Agent.Server Connecting to the Mlos.Optimizer Starting target/bin/Release/x86_64/SmartCache observations: 0 info: Microsoft.Hosting.Lifetime[0] Now listening on: http://[::]:5000 info: Microsoft.Hosting.Lifetime[0] Application started. Press Ctrl+C to shut down. info: Microsoft.Hosting.Lifetime[0] Hosting environment: Production info: Microsoft.Hosting.Lifetime[0] Content root path: /src/MLOS Starting Mlos.Agent Waiting for Mlos.Agent to exit Found settings registry assembly at target/bin/Release/AnyCPU/SmartCache.SettingsRegistry.dll Waiting for agent to respond with a new configuration. Register { \u0026#34;cache_implementation\u0026#34;: \u0026#34;LeastRecentlyUsed\u0026#34;, \u0026#34;lru_cache_config.cache_size\u0026#34;: 100 } HitRate = 0 Suggest False {\u0026#34;cache_implementation\u0026#34;: \u0026#34;LeastRecentlyUsed\u0026#34;, \u0026#34;lru_cache_config.cache_size\u0026#34;: 747} observations: 1 Waiting for agent to respond with a new configuration. Register { \u0026#34;cache_implementation\u0026#34;: \u0026#34;LeastRecentlyUsed\u0026#34;, \u0026#34;lru_cache_config.cache_size\u0026#34;: 747 } HitRate = 0 Suggest False {\u0026#34;cache_implementation\u0026#34;: \u0026#34;MostRecentlyUsed\u0026#34;, \u0026#34;mru_cache_config.cache_size\u0026#34;: 2554} observations: 2 ... Explanation The Mlos.Agent that gets started by the Mlos.Agent.Server waits for a signal that the component (SmartCache) has connected to the shared memory region before starting to poll the component for messages to process. This is important in case the component is started independently.\nIn this case, Mlos.Agent.Server itself starts the component.\nOnce started, SmartCache will attempt to register its component specific set of shared memory messages with the Mlos.Agent in the Mlos.Agent.Server using RegisterComponentConfig and RegisterAssemblyRequestMessage from Mlos.Core and Mlos.NetCore. That includes the name of the SettingsRegistry assembly (.dll) corresponding to that component\u0026rsquo;s settings/messages.\nThe Mlos.Agent.Server needs to be told where it can find those assemblies in order to load them so that it can parse the messages sent by the component. To do that, we use the --settings-registry-path option. We also need to provide the path to a corresponding ExperimentSession dll in the --experiment option in order to tell the agent how to process those messages for this experiment.\nIn the second example we also tell the Mlos.Agent.Server how to connect to the (Python) MLOS Optimizer Service over GRPC so that the application message handlers setup by the SmartCache.SettingsRegistry for the agent can request new configuration recommendations on behalf of the application.\nFor additional details, see:\n Mlos.Agent.Server/MlosAgentServer.cs SmartCache/Main.cpp SmartCache.ExperimentSession/SmartCacheExperimentSession.cs SmartCache.SettingsRegistry/Codegen/SmartCache.cs  See Also For additional implementation details and demonstration code, please see the SmartCache CPP Notebook\n"});index.add({'id':27,'href':'/MLOS/source/Examples/SmartSharedChannel/','title':"Smart Shared Channel",'section':"Examples",'content':"SmartSharedChannel Example This SmartSharedChannel example demonstrates a using a microbenchmark of the MLOS shared memory channel to tune the MLOS shared memory channel itself.\nHere are some brief instructions on how to try it out:\nBuilding  Note: these command examples expect to be run from the top-level of the repository.\nTo move there, execute the following:\ncd $(git rev-parse --show-toplevel)\n   Build or pull the docker image\n  Create a docker image instance\n  Build the code\n(inside the docker container)\nmake -C source/Mlos.Agent.Server make -C source/Examples/SmartSharedChannel all install   Executing dotnet target/bin/Release/AnyCPU/Mlos.Agent.Server/Mlos.Agent.Server.dll \\  --executable target/bin/Release/x86_64/SmartSharedChannel \\  --settings-registry-path target/bin/Release/AnyCPU "});index.add({'id':28,'href':'/MLOS/source/Mlos.Agent.Server/','title':"Mlos. Agent. Server",'section':"Sources",'content':"Mlos.Agent.Server Contents  Mlos.Agent.Server  Contents Overview Building  Linux Windows   Executing Caveats    Overview The Mlos.Agent.Server is essentially a small and generic C# wrapper around several other communication channels to allow different components to connect for component experimentation convenience.\nIt provides\n  Shared memory communication channels via the Mlos.Agent and Mlos.NetCore libraries.\n  A Mlos.Agent.GrpcServer GRPC channel to allow driving the experimentation process from a Jupyter notebook (work in progress).\n  A GRPC client to allow connecting to the (Python) mlos.Grpc.OptimizerServicesServer to store and track those experiments.\n  Since it is meant as a reusable agent for different components, it contains no specific message processing logic itself.\nRather, it starts an Mlos.Agent message processing loop which loads each component\u0026rsquo;s SettingsRegistry assembly dlls upon registration request (via the RegisterComponentConfig and RegisterAssemblyRequestMessage from Mlos.Core and Mlos.NetCore) and uses the specified ExperimentSession class to setup the message callbacks that handle compnent and experiment specific things like aggregating and summarizing telemetry messages, interfacing with the optimizer using different parameter and/or context search spaces, and relaying component reconfiguration requests, etc.\nSee the SmartCache code, especially its ExperimentSession class for a more detailed example.\nBuilding See Also\n General build instructions   Note: these commands are given relative to the root of the MLOS repo.\n Linux make -C source/Mlos.Agent.Server Windows msbuild /m /r source/Mlos.Agent.Server/Mlos.Agent.Server.csproj Executing The Mlos.Agent.Server has several different operating modes. Please see the --help usage output for additional details.\ndotnet target/bin/Release/Mlos.Agent.Server.dll --help Caveats  The system currently only supports a single tunable process at a time. Multiple agent/target-process pairs support (e.g. via configurable unix socket domain path for anonymous shared memory exchange) are planned for future development.  "});index.add({'id':29,'href':'/MLOS/source/Mlos.Core/doc/','title':"Doc",'section':"Sources",'content':"Mlos.Core Documentation The Mlos.Core library\u0026rsquo;s primary purpose is to provide shared memory communication channel abstractions for the target application to communicate with the external agent (Mlos.Agent.Server) to register and expose its components\u0026rsquo; settings, their telemetry messages, and their feedback callback actions for tuning them.\nSee Also:  Mlos Shared Memory Communication Channel  "});index.add({'id':30,'href':'/MLOS/source/Mlos.Core/doc/SharedChannel/','title':"Shared Channel",'section':"Doc",'content':"MLOS Shared Channel This document describes the implementation details of the mechanism (a shared memory communication channel) used for a target system to communicate with an external agent for tuning it.\nFor additional context, please see the MlosArchitecture.md documentation.\nSee source/Mlos.Core to browse the code.\nContents  MLOS Shared Channel  Contents Shared Channel  Principles Circular buffer algorithm  Writer Reader Writer continued Reader continue Cyclic buffer handling   Scaling out readers   Shared channel implementation  Diagram Policies Notes      Shared Channel A shared channel is a one-directional communication channel based on a single shared memory block (i.e. between processes). It supports multiple concurrent writers and readers.\nIts purpose is to allow exposing information from a target system to an external agent (e.g. Mlos.Agent) and providing feedback from that external agent to control the target system\u0026rsquo;s tunables.\nTypically there are several shared channels in each system:\n A control channel for registering settings to setup up additional channels. One or more (e.g. for each tunable component) telemetry and feedback channel pairs.  Messages are exchanged on a shared channel as Frames. Frames can be variable length (e.g. if they include variable length data like strings) or fixed length (e.g. just numerical data). The format of each frame is code generated by Mlos.SettingsSystem.CodeGen from annotated C# data structures specified in a SettingsRegistry provided by the developer. See Mlos Settings System Code Generation more details on the code generation and settings system.\nPrinciples The shared channel is comprised of four contiguous memory regions (though at times some of them could have zero size):\n  FreeRegion - a writer can acquire the region by (atomically) saving the original WritePosition, and advancing WritePosition by the frame size. The writer can then write the frame at the WriteOffset = (original WritePosition) % Buffer.Size.\n  DirtyRegion - contains already processed (read) frames that are ready to be reclaimed (as indicated in the frame\u0026rsquo;s header). Writers are responsible for reclaiming frames and advancing the FreePosition.\n  ActiveReadsRegion - a memory region between the start of the oldest unprocessed frame start and the ReadPosition (logically) or ReadOffset (physically). Readers are actively processing messages in this region.\n  ActiveWritesRegion - a memory region between the ReadPosition and WritePosition (logically) or equivalently between ReadOffset and WriteOffset (physically). This region contains both:\n Frames that have already been written but have not been processed yet, and Frames that are being written by the writers.  These two types of frames are differentiated by information contained in their headers (described below).\n  At the logical level the boundaries between the regions are controlled by three position variables that are atomically updated:\n WritePosition ReadPostion FreePosition  They are all 32-bit unsigned integers that are monotonically increasing except for integer overflows. Note that integer overflows are part of the design and do not affect correctness.\nAt the physical level the boundaries between the regions are controlled by corresponding offsets into the circular buffer:\n WriteOffset = WritePosition % Buffer.Size ReadOffset = ReadPosition % Buffer.Size FreeOffset = FreePosition % Buffer.Size    Advancing positions is implemented with atomic CPU operations (e.g. std::atomic::compare_exchange or Interlocked.CompareExchange).\n  Reader and writer threads must use these to first acquire a region before touching the memory inside it. There are two exceptions to this rule:\n when thread reads from memory in unknown state cleanup does not require acquire, it just cleans up (atomically) as far as it\u0026rsquo;s safe to do so    The following invariant ensures that the ActiveWritesRegion will never overlap the ActiveReadsRegion \\\nFreePosition \u0026lt;= ReadPosition \u0026lt;= WritePosition \u0026lt; FreePosition + Buffer.Size\n  Buffer.Size must be a power of two (2N)\n  Circular buffer algorithm Writer Writer threads expand an ActiveWriteRegion by atomically exchanging WritePosition by a frame length. If it succeeds, it means the writer has acquired the memory region and can write the frame. The ActiveWriteRegion will never overlap with ActiveReadsRegion. The writer ensures there is a minimum gap of the size of FrameHeader between FreeOffset and WriteOffset. The writer stores the frame payload and atomically updates the frame length.\nReader By default reader threads spin [1] on the ReadOffset which always points to the length of the next written frame. When the value becomes available (Frame.Length \u0026gt; 0), it tries to acquire the region by atomically exchanging ReadPostion. If the compare and exchange fails, it means another reader thread is already processing the frame. When it succeeds, the reader should call the proper dispatcher routine to process the contents of the frame. After processing the frame, the reader clears the frame payload and atomically updates the Frame.Length to be negative [2], indicating that it is available to be written again.\n [1] Alternative policies can be specified to control sleeping vs spinning behavior.\n[2] It is possible that the reader crashes between these steps. We use reference counting to support detecting these situations.\n Writer continued Now, the memory occupied by the frame the reader finished with is clear, except for the negative Frame.Length. Writer threads use negative frame lengths as a hint to advance FreePosition until DirtyRegion has a minimal size (according to the invariant above).\nReader continue When the reader waits on the ReadOffset, it reads from the memory region that has not been acquired. This is the FreeRegion, and the memory has been cleared by the previous reader - except for the negative Frame.Length. The Offset of the frame is always aligned to sizeof(uint32_t), so if the current ReadOffset is the same as the offset of the old processed frame, the reader will read a negative value. So, the reader will spin until it reads positive frame length.\nCyclic buffer handling If a writer is unable to write a full frame (i.e. the end of the frame is greater than buffer margin), it will write an empty frame (no payload) just to advance Position to the beginning of the buffer.\nReaders can identify these empty frames using their CodeGenTypeIndex header, and pass over them.\nScaling out readers To scale out the number of reader threads, we introduced a control bit which defines if the frame has been fully written. The control bit (Done in the diagram above) is the lowest bit in the frame length field.\nWith the modified algorithm, the writer stores the length of the frame with the Done bit set to 0. This allows one reader to acquire the current frame region while other readers threads can advance and wait for the next frame.\nShared channel implementation Diagram Policies The implementation allows the use of different policies using the same shared memory buffer instance.\nPolicies are responsible for:\n error handling cross process notification handling full buffer spinning implementation  The shared channel requires two policies.\n  TSpinPolicy is responsible for the spin/wait algorithm when a frame is not available.\n  TChannelPolicy implements error handling code and cross-process notification.\n  TSpinPolicy is a local variable in functions that require spin functionality whereas TChannelPolicy is a field in SharedChannel class.\nWe do not share TSpinPolicy across multiple writers, so each thread creates its own object. The SharedChannel contains a single instance TChannelPolicy which contains a synchronization primitive used to signal remote process when there is a new frame.\nNotes The implementation of shared channel is heavily influenced by C# metaprograming:\nFederico Lois — Metaprogramming for the masses\n"});index.add({'id':31,'href':'/MLOS/source/Mlos.Core/doc/SharedMemoryManagement/','title':"Shared Memory Management",'section':"Doc",'content':"Shared Memory Management Bootstrap sequence Overview Target process, process which hold tunable components.\nMLOS Agent - the agent process, responsible for collecting the telemetry and communicating with the optimizer.\nOS implementation differences. MLOS is handling shared memory differently on Windows and Linux.\nLinux is creating anonymous shared memory, and it is passing a file descriptor between the processes using Unix domain socket.\nThe target process is responsible for creating shared memory regions and channel synchronization primitives. Once the shared memory is created, the target process will connect to the Unix socket and sends the shared memory file descriptors. Once the agent obtains all the required shared memory descriptors, the agent will start processing shared channel messages.\nOn startup, the target process creates a thread which is responsible for handling agent request. If the agent restarts, it will notify the target process that it needs a list of file descriptors once the target process reconnects to the socket.\nImplementation details SharedMemoryMapView SharedMemoryRegionView "});index.add({'id':32,'href':'/MLOS/source/Mlos.NetCore/Doc/SharedChannelScaleout/','title':"Shared Channel Scaleout",'section':"Sources",'content':"Shared Channel Scaleout Document contains results of the improvement in the shared communication channel. We specifically address lack of the scalability issue. The channel does not scale well with increasing number of readers and writers operating on the same channel instance.\nBenchmark The benchmark is implemented in Mlos.NetCore.Benchmark project\nBenchmark:\n $(MLOSROOT)\\MLOS\\out\\obj\\Source\\Mlos.NetCore.Benchmark\\obj\\amd64\\Mlos.NetCore.Benchmark.exe -i \u0026ndash;filter SharedChannelReaderScaleOutBenchmarks\n Results Hyper-threading is disabled.\n Intel E5-2670 v3 @ 2.30 GHz:    ReaderCount Mean[B] Mean[I] Error[B] Error[I] StdDev[B] StdDev[I] Allocated     1 830.4 ms 712.9 ms 9.73 ms 5.02 ms 9.10 ms 4.70 ms 3584 B   2 1,170.8 ms 924.3 ms 3.56 ms 2.88 ms 3.33 ms 2.55 ms 2104 B   4 1,565.5 ms 1,084.0 ms 1.19 ms 12.20 ms 1.05 ms 11.41 ms 3856 B   8 2,213.2 ms 1,289.8 ms 2.14 ms 15.75 ms 2.00 ms 14.73 ms 8048 B   12 4,958.1 ms 1,840.7 ms 13.53 ms 16.53 ms 11.99 ms 15.46 ms 11216 B   16 7,824.5 ms 1,969.9 ms 13.16 ms 19.01 ms 10.99 ms 15.87 ms 14944 B   20 8,261.5 ms 2,363.7 ms 21.81 ms 46.70 ms 20.40 ms 53.78 ms 18576 B   24 8,605.7 ms 2,386.9 ms 42.37 ms 23.49 ms 39.63 ms 21.98 ms 21376 B    Amd Ryzen 2700X    ReaderCount Mean[B] Mean[I] Error[B] Error[I] StdDev[B] StdDev[I] Allocated     1 541.7 ms 510.1 ms 2.62 ms 8.04 ms 2.32 ms 7.52 ms 2856 B   2 704.1 ms 526.2 ms 3.03 ms 6.67 ms 2.53 ms 6.24 ms 2064 B   4 2,724.8 ms 687.1 ms 20.13 ms 13.38 ms 18.83 ms 13.74 ms 3856 B   8 2,743.6 ms 795.1 ms 22.48 ms 15.40 ms 21.02 ms 18.34 ms 6616 B    Improvements Avoid creating proxies in the loop Creating proxy classes (in this example .Sync and .ReadPosition) has a significant cost when the code is running in a tight loop. Before entering the loop, create a proxy structure and access it later inside the loop.\nExample:\nreadPosition = buffer.Sync.ReadPosition.Load();\rIs replaced with:\nMLOSProxy.ChannelSynchronization sync = buffer.Sync;\rStdTypesProxy.AtomicUInt32 atomicReadPosition = sync.ReadPosition;\r...\rint spinIndex = 0;\rwhile (true)\r{\r// Wait for the frame become available.\r // Spin on current frame (ReadOffset).\r //\r readPosition = atomicReadPosition.Load();\rStdAtomic Removed MemoryBarrier and replaced with Volatile.Read and Volatile.Write.\nreadPosition = buffer.Sync.ReadPosition.Load();\r...\rInterlocked.MemoryBarrier();\rreturn *(uint*)Buffer.ToPointer();\r00007FFB118BC150 50 push rax\r00007FFB118BC151 4C 8D 41 18 lea r8,[rcx+18h]\r00007FFB118BC155 49 8B C0 mov rax,r8\r00007FFB118BC158 48 8B 00 mov rax,qword ptr [rax]\r00007FFB118BC15B F0 83 0C 24 00 -\u0026gt;lock or dword ptr [rsp],0\r00007FFB118BC160 44 8B 08 mov r9d,dword ptr [rax]\r00007FFB118BC163 41 8B C1 mov eax,r9d\rMLOSProxy.ChannelSynchronization sync = buffer.Sync;\rStdTypesProxy.AtomicUInt32 atomicReadPosition = sync.ReadPosition;\r...\rreadPosition = atomicReadPosition.Load();\r...\rreturn Volatile.Read(ref *ptr);\rreadPosition = atomicReadPosition.Load();\r 00007FFDD883B005 48 8B C7 mov rax,rdi 00007FFDD883B008 8B 28 mov ebp,dword ptr [rax] 00007FFDD883B00A 8B C5 mov eax,ebp "});index.add({'id':33,'href':'/MLOS/source/Mlos.Notebooks/RustcFlagOptimizationDemo/','title':"Rustc Flag Optimization Demo",'section':"Sources",'content':"Rustc Flag Optimization Demo Author: Zack Porter (zaporter) 2021-06-07\nThis project demonstrates the ability to optimize rust compliation times with MLOS.\nDirectory structure:\n RustcFlagOptimization.ipynb - Jupyter notebook of the project RustcFlagOptimization.html - HTML rendering of the Jypyter notebook for portability reasons (2021-06-07) bevybench - rust benchmark to use as an example to optimize compilation with MLOS. Run via cargo run optimizer_done_1000.obj - Pickled python optimizer object after having done 1000 runs observations.csv - Observations from the optimizer exported via df.to_csv(..)  "});index.add({'id':34,'href':'/MLOS/source/Mlos.Python/Docker/','title':"Docker",'section':"Sources",'content':"Mlos Python Docker Notes We use docker to package Mlos and its Python dependencies together with a database instance (in this case SqlServer) for storing optimizer models and experiment data.\nThis document contains a few brief notes on that setup.\nCreating a secrets file Inside the Secrets directory there is a sample_docker_connection_string.json. It is complete except for the Password field, which we don\u0026rsquo;t want checked into the repository.\n  Create a good password.\n  Make a copy of that file at local_docker_connection_string.json\n  cp sample_docker_connection_string.json local_docker_connection_string.json\rEdit the new file to include your new password:  {\r\u0026#34;Host\u0026#34;: \u0026#34;.\u0026#34;,\r\u0026#34;Username\u0026#34;: \u0026#34;\u0026#34;,\r\u0026#34;Password\u0026#34;: \u0026#34;\u0026#34;,\r\u0026#34;DatabaseName\u0026#34;: \u0026#34;MlosModels\u0026#34;,\r\u0026#34;TrustedConnection\u0026#34;: false,\r\u0026#34;Driver\u0026#34;: \u0026#34;ODBC Driver 17 for SQL Server\u0026#34;\r}\rBuilding the image For the SA_PASSWORD below use the same password as the one used in the secrets file above.\ncd $MLOS_ROOT/source/Mlos.Python\rdocker build -f Docker/Dockerfile -t mssql-server-linux-with-mlos-python --build-arg SA_PASSWORD=*YouKnowWhatToPutHere* .\r Note: the trailing . in the preceeding command is important.\n Other useful commands   List images:\ndocker images:\r  Start a container named MlosOptimizerService from the image we built earlier:\ndocker run -p1433:1433 --name MlosOptimizerService mssql-server-linux-with-mlos-python\r  Connect to the container named MlosOptimizerService:\ndocker exec -it MlosOptimizerService /bin/bash\r  Stop the container named MlosOptimizerService:\ndocker stop MlosOptimizerService\r  Remove that container:\ndocker container rm MlosOptimizerService\r  Notes and considerations We initialize the schema at build time of the docker image so that the containers can be stopped and restarted without wiping all the data from them.\nFor that we need a way to:\n Start SqlServer from within the Dockerfile Execute sqlcmd to create the schema Stop SqlServer All of this happens in the Dockerfile during the docker build process.  An alternative is to use persistent volumes. For instance:\n Create a persistent volumes:  docker volume create MlosOptimizerService\rUse it during docker container creation:  docker run -p1433:1433 --name MlosOptimizerService --volume MlosOptimizerService:/var/opt/mssql mssql-server-linux-with-mlos-python\r"});index.add({'id':35,'href':'/MLOS/source/Mlos.Python/Docs/BayesianOptimizerArchitecture/','title':"Bayesian Optimizer Architecture",'section':"Sources",'content':"Bayesian Optimizer Architecture Components  Surrogate Models Utility Functions Numeric Optimizers Experiment Designer Bayesian Optimizer  Other Classes  Hypergrids Optimization Problems  Hypergrids Hypergrids are used to describe multidimensional spaces comprized of Continuous, Discrete, Ordinal, or Categorical dimensions.\nAll optimizers we have reviewed to date (Hypermapper, SMAC, bayesopt, and scikit-learn) optimizers implement their own notion of a search space.\nHypergrids are meant to provide a superset of their functionalities. They further allow us to express hierarchical search spaces, where some parameters only become meaningful if some other parameter (e.g. a boolean flag) is set.\nOptimization Problems OptimizationProblem is a class describing:\n The parameter space (aka decision variables) - represented as a Hypergrid of all allowed parameter values. The objective space - represented as a Hypergrid of all objectives we may wish to optimize The context space (akin to controlled variables)  Surrogate Models The role of surrogate models is to use parameter values and context values to predict performance metrics.\n"});index.add({'id':36,'href':'/MLOS/source/Mlos.Python/Docs/BayesianOptimizerV1/','title':"Bayesian Optimizer V1",'section':"Sources",'content':"Bayesian Optimizer V1 The goal of this document is to describe the architecture and inner workings of a Bayesian Optimizer.\nComponents The following components will be necessary:\n Experiment Observation Storage (Table in SQL Server) Bayesian Optimizer Surrogate Models  Architecture TODO: describe how we will use the technologies:\n Docker (K8S?) SQL Server Python ML.Net SQL DB RPC vs. gRPC  Obviously an experiment and the corresponding bayesian optimizer have to be compatible so that the observations generated by the experiment are within the optimizers observation space. Thus the Experiment and the Optimizer should be configured together. One way to accomplish this is to have the Experiment own an Optimizer.\nAn experiment has to further interact with the target. There are two general approaches:\n Active Learning Online Learning  Active vs. Online Learning Active Learning In this mode the experiment controls the deployment and the workload. In other words, the experiment can restart the target with a desired configuration, in a desired context, with a desired workload.\nA mental shortcut: we would use this mode in the lab to train models.\nOnline Learning In this mode the experiment can observe the target (its deployment context, and performance metrics) but is only allowed to change the configuration.\nA mental shortcut: we would use this mode in production.\nExplore vs. Exploit While we are talking about training modes, another dichotomy should be made clear.\nWe would run the optimizer in an Explore mode in the lab or on a \u0026lsquo;B\u0026rsquo; instance of an A/B testing setup. We would only ever run the optimizer in the Exploit mode on production deployments, but that\u0026rsquo;s the song of distant future.\nThe mechanism for controlling the behavior would be to adjust the acquisition function.\nExplore In Explore mode the optimizer\u0026rsquo;s objective is to explore the search space with the goal of sampling a variety of configurations and building robust surrogate models. This comes at a risk of drastically regressing performance or even crashing the server, but we are OK with that in the lab setting. The payoff is that the optimizer builds models that more completely understand the feasibility region as well as the performance of the target under various configurations.\nExploit In Exploit mode the optimizer\u0026rsquo;s objective is to conservatively adjust target\u0026rsquo;s configuration to maximize performance in a given workload. What this means is that the optimizer would only issue a reconfiguration recommendation if it has a very low probability of deteriorating performance.\nStorage Observations obtained from the target should be passed to the optimizer, but additionally should be stored so that we could experiment with a variety of models. For now, we will use SQL Server as storage for observations, though we could extend it to any other technology as long as required connectors are available / can be written.\nBayesian Optimizer Anatomy of a Bayesian Optimizer A Bayesian Optimizer (BO) consists of two parts, both of which can be interchanged to achieve a variety of goals. Thes are:\n An acquisition function A surrogate model or surrogate models ensemble  Simple optimization flow Below is a sequence diagram depicting a simple bayesian optimization process.\nsequenceDiagram\rparticipant User\rparticipant OptimizationTarget\rparticipant Optimizer\rparticipant SurrogateModel\rparticipant AcquisitionFunction\rUser -\u0026gt;\u0026gt; OptimizationTarget: Start(defaultConfig)\ractivate OptimizationTarget\rloop while optimizationBudget \u0026gt; 0\rOptimizationTarget -\u0026gt;\u0026gt; Optimizer: observations\rNote over Optimizer, SurrogateModel: regression model is fit to data\rOptimizer -\u0026gt;\u0026gt; SurrogateModel: fit(observations) User -\u0026gt;\u0026gt; Optimizer: SuggestNewConfig()\rOptimizer -\u0026gt;\u0026gt; Optimizer: CandidateConfigs = SearchSpace.Meshgrid()\rloop for config in CandidateConfigs\rOptimizer -\u0026gt;\u0026gt; SurrogateModel: mean, stdev = predict(config)\rOptimizer -\u0026gt;\u0026gt; AcquisitionFunction: score = AcquisitionFunction(mean, stdev)\rNote over Optimizer, AcquisitionFunction: if score \u0026gt; maxScore: maxScore = score, bestConfig = config\rend\rOptimizer -\u0026gt;\u0026gt; User: bestConfig\rUser -\u0026gt;\u0026gt; OptimizationTarget: reconfigure(bestConfig)\rend\rUser -\u0026gt;\u0026gt; OptimizationTarget: Stop()\rdeactivate OptimizationTarget\rWhile this process is quite naive, it allows us to discuss the way that a Bayesian Optimizer works.\n The user starts the OptimizationTarget which emits telemetry. Automagically (in a way irrelevant to our discussion) that telemetry is converted to observations and passed to the optimizer. The optimizer uses all observations to fit a surrogate model (let\u0026rsquo;s assume it\u0026rsquo;s a Gaussian Process). Eventually, the user is ready to try a new configuration and so the user asks the optimizer for a new config to try. The optimizer generates a meshgrid of possible configurations. The model is asked to predict performance and uncertainty for each of the points in the grid. Those predictions are passed through an acquisition function which returns some score. That function could be: probability of improvement, expected improvement, lower confidence bound, or even an inverse of the probability of catastrophic deterioration. This choice depends on our objective and our constraints. More on that later. The optimizer selects the configuration with the highest score and returns to the user. The user reconfigures the target and the story repeats until we run out of budget.  The Role of the Surrogate Model That role is very simple: predict performance based on configuration. Ideally, a surrogate model should report it\u0026rsquo;s level of uncertainty in the prediction.\nTODO: feasibility predictors\nThe Role of the Acquisition Function The role of the acquisition function is difficult to overstate. It is this function that determines whether the optimizer is in an Explore vs. Exploit mode. An acquisition function can score each candidate configuration in a number of ways: it can favor configurations close to existing observations so that the risk is minimized. It can favor promising and uncertain configurations to favor exploration.\nSome of the popular choices:\n Probability of Improvement (POI) - maximizes the chance that a new point will be better than the observations we have seen so far. Expected Improvement (EI) - will select points maximize expected improvement. It much less conservative than POI. POI would select points with high chance of improvement, even if the improvement is infinitesimal. EI will select points so that the improvement is as large as possible. Lower Confidence Bound - will select poinst with the highest lower confidence bound. In a way it is a strategy that is motivated by avoiding risk rather than winning big. Safe Optimization - this function will attempt to minimize the probability of suggesting a catastrophic configuration. In other words: maximize the probability of safety.  Each of the above functions can be further parameterized to tune it to our needs. It is a very rich selection and should be addressed by hyper-parameter tuning and AutoML initiatives.\nMaximizing Acquisition Function In our naive example above, we used grid search maximize the value of hte Acquisition Function. That is obviously wasteful especially when we realize that we will repeat this step over and over again, for every new suggestion (iteration of the outer loop). Several observations can prove useful:\n The surrogate model changes only slightly between iterations and those changes are most pronounced in the vicinity of the new observation. It stands to reason that between iterations points that had a high acquisition function value are likely to stay promising, and points that had a low acquisition value function are likely to remain unpromising. With the exception of points in the vicinity of any new observations.  We could use that insight to jump-start our optimization by first considering points that have been promising in the past.\nFurthermore, some surrogate model classes can encode clues to their maxima. For example: in a Gaussian Process maxima are likely to occur in the vicinity of the kernels.\nMoreover, a plethora of optimization algorithms have been developed over the centuries that should perform better than grid search (especially for smooth functions such as Neural Networks and Gaussian Processes sometimes are). There are direct methods, gradient based methods (first and second order), population methods and even surrogate methods (\u0026hellip; though that last suggestion is bound to raise eye-brows - but that alone should not disqualify it :)).\nThe point is, by using some of those methods, we are likely to converge on optima faster and save a lot of compute on model scoring.\n"});index.add({'id':37,'href':'/MLOS/source/Mlos.Python/Docs/OptimizerMonitoring/','title':"Optimizer Monitoring",'section':"Sources",'content':"Optimizer Monitoring Motivation The goal of this document is to outline the process of monitoring the optimizers, enumerate the metrics we wish to collect and the tools required to do so.\nWhat we wish to monitor For each optimizer we should be able to:\n View it\u0026rsquo;s current configuration (done). View all the data that it was trained on. View the state of the surrogate models:  Have they been fitted? How many times has it been refitted? In case of an ensemble: has each of the component models been fitted? What are the values of goodness of fit measures? How many samples did the model consume? What exact data was each model trained on? Which of the objectives is the model predicting? Data specific to each model class. For example for a Decision Tree:  Number of leafs Leaf statistics (sample mean, sample var, count) Depth and shape Splits      It makes sense for all of that information (except for the exact data) to be maintained by the model, and presented upon request. Storing multiple copies of data seems to be heavy overhead, so we should avoid it if at all possible. This problem will be solved by the introduction of DataSets and DataSetViews.\nRepresenting the state It might be tempting to create a dictionary to store all of this data. I think such hash-map-oriented programming paradigm will quickly deteriorate, especially as more and more people begin contributing.\nSo a better approach would be to have a base class that stores (in common format) information common to all model classes. For each model we could derive a specialized class, that stores the more specific information.\nSerializing the state We will have to be able to send this data over gRPC, so we must be mindful of how to serialize/deserialize it. Options include:\n Defining this struct in a .proto file and populating it directly. Serializing the struct to json. Serializing the struct using pickle.  The problem with .proto approach is that this becomes part of the API and will make changing anything unnecessarily hard.\nThe problem with json is that writing json serializer is extra work.\nThe problem with pickle is that only Python can deserialize it. But since we only need to deserialize in Python for the foreseeable future, we will go with this option. However, to hedge our bets, I\u0026rsquo;ll wrap the calls to pickle.dumps and pickle.loads inside a .serialize(), .deserialize() functions to allow for an implementation change down the road (e.g. if we want to monitor the optimizer from C# or Julia).\n"});index.add({'id':38,'href':'/MLOS/source/Mlos.Python/Docs/SimpleImportGraph/','title':"Simple Import Graph",'section':"Sources",'content':"digraph dfd2 {\rnode[shape=record]\r\u0026quot;BayesianOptimizer.py\u0026quot; -\u0026gt; \u0026quot;OptimizationProblem.py\u0026quot;\r\u0026quot;HomogeneousRandomForestRegressionModel.py\u0026quot; -\u0026gt; \u0026quot;DecisionTreeRegressionModel.py\u0026quot;\r\u0026quot;BayesianOptimizer.py\u0026quot; -\u0026gt; \u0026quot;OptimizerInterface.py\u0026quot;\r\u0026quot;ExperimentDesigner.py\u0026quot; -\u0026gt; \u0026quot;RegressionModel.py\u0026quot;\r\u0026quot;OptimizerInterface.py\u0026quot; -\u0026gt; \u0026quot;OptimizationProblem.py\u0026quot;\r\u0026quot;BayesianOptimizer.py\u0026quot; -\u0026gt; \u0026quot;ExperimentDesigner.py\u0026quot;\r\u0026quot;HomogeneousRandomForestRegressionModel.py\u0026quot; -\u0026gt; \u0026quot;Prediction.py\u0026quot;\r\u0026quot;DecisionTreeRegressionModel.py\u0026quot; -\u0026gt; \u0026quot;RegressionModel.py\u0026quot;\r\u0026quot;ExperimentDesigner.py\u0026quot; -\u0026gt; \u0026quot;ConfidenceBoundUtilityFunction.py\u0026quot;\r\u0026quot;RandomSearchOptimizer.py\u0026quot; -\u0026gt; \u0026quot;OptimizationProblem.py\u0026quot;\r\u0026quot;HomogeneousRandomForestRegressionModel.py\u0026quot; -\u0026gt; \u0026quot;RegressionModel.py\u0026quot;\r\u0026quot;DecisionTreeRegressionModel.py\u0026quot; -\u0026gt; \u0026quot;Prediction.py\u0026quot;\r\u0026quot;ExperimentDesigner.py\u0026quot; -\u0026gt; \u0026quot;RandomSearchOptimizer.py\u0026quot;\r\u0026quot;ExperimentDesigner.py\u0026quot; -\u0026gt; \u0026quot;OptimizationProblem.py\u0026quot;\r\u0026quot;BayesianOptimizer.py\u0026quot; -\u0026gt; \u0026quot;HomogeneousRandomForestRegressionModel.py\u0026quot;\r}\r"});index.add({'id':39,'href':'/MLOS/source/Mlos.Python/mlos/Examples/SmartCache/OverviewOfCachingStrategies/','title':"Overview of Caching Strategies",'section':"Sources",'content':"Overview of Caching Strategies and Algorithms Goal The objective of this document is to describe various approaches to caching. The intent is to then implement a number of paramiterized caching algorithms and allow MLOS to select between them on a per-workload basis.\nLinks A loose list of sources consulted in this survey:\n https://medium.com/datadriveninvestor/all-things-caching-use-cases-benefits-strategies-choosing-a-caching-technology-exploring-fa6c1f2e93aa https://en.wikipedia.org/wiki/Cache_replacement_policies  Potential Objectives  average retrieval time/cost/latency (or other statistics: percentiles, CI\u0026rsquo;s etc) hit ratio and miss ratio cache hit latency metrics (in case of a hit) data staleness metrics (distribution of time since last usage among all cache entries)  Plausible Implementations/Approaches  FIFO - a queue and a hash-map would work. Parameters: max_num_entries, max_size LIFO - a stack and a hash-map would work. Parameters: max_num_entries, max_size LRU - a linked list and a hash-map. Parameters: max_num_entries, max_size. There are variants descirbed here: https://en.wikipedia.org/wiki/Page_replacement_algorithm#Variants_on_LRU TLRU - time aware LRU or LRU with TTL MRU - most recently used. Apparently useful for random access patterns and cyclical access patterns PLRU - pseudo-LRU. A set of heuristic and probability based approaches approximating LRU Random Replacement - we\u0026rsquo;d better be able to beat that Segmented LRU (SLRU) - a more complex strategy described in detail here: https://en.wikipedia.org/wiki/Cache_replacement_policies  "});index.add({'id':40,'href':'/MLOS/source/Mlos.Python/mlos/Spaces/HypergridAdapters/AboutHypergridAdapters/','title':"About Hypergrid Adapters",'section':"Sources",'content':"Hypergrid Adapters Motivation Categorical to numeric projections The goal of adapters is to make a broader class of hypergrids compatible with any surrogate model. The chief problem in absence of adapters is that some models (RERF, DecisionTreeRegressionModel) can only operate on numeric datatypes, but the configuration for many components includes strings, and booleans as well. A suitable adapter will map such categorical dimensions into numeric ones and allow transparent projection of observations and suggestions between the components and the regression models.\nRenames and hierarchy flattening Another use case is that from the smart components perspective hypergrids can be hierarchical, but some models can only work with flat data. A suitable adapter will flatten the hierarchy when feeding data to the model, and rebuild the hierarchy when emitting suggestions/predictions from the model.\nImputing values Some models work well with missing values, but some models cannot handle them. An imputing adapter could be used to impute values according to a specified rule. We will start with a constant, but more sophisticated imputing strategies will be enabled.\nSkipping dimensions Sometimes we would like to filter out dimensions. One way to do it is to use an adapter.\nTransformations on input space PCA etc. standardizing/normalizing the input space.\nRequirements  An adapter has to derive from the Hypergrid base class (and thus implement its interface). They expose the interface of the target. Adapters must be stackable - we should be able to apply a renaming adapter on top of an imputing adapter. Adapters must maintain all mappings they create and we must be able to serialize/deserialize adapters objects. Adapters must be able to project Point and pandas.DataFrame objects. We need json encoders/decoders for each or maybe pickle is enough\u0026hellip;  "});index.add({'id':41,'href':'/MLOS/source/Mlos.SettingsSystem.Attributes/','title':"Mlos. Settings System. Attributes",'section':"Sources",'content':"MLOS Settings System Attributes This project contains the base classes used by both\n Client code to specify their SettingsRegistry\u0026rsquo;s. The SettingsSystem CodeGen tools to evaluate client code provided SettingsRegistry\u0026rsquo;s.  The Attributes directory contains the core C# Attribute classes used to help annotate user provided SettingsRegistry classes.\nNote: there are also non-Settings that are code generated from these attributes such as messages on the shared memory channels.\nSee Also  MLOS Settings System Code Generation documentation  "});index.add({'id':42,'href':'/MLOS/source/Mlos.SettingsSystem.CodeGen/','title':"Mlos. Settings System. Code Gen",'section':"Sources",'content':"MLOS Settings System Code Generation This document provides some documentation on the implementation internals of the MLOS Settings System Code Generation process.\nContents  MLOS Settings System Code Generation  Contents Why code generation? Why custom CodeGen?  Performance Then why not XEvents? Benefits   Build process Classes Hierarchy of CodeWriters  Namespaces   Basic structure generation  CppObjectCodeWriter CppProxyCodeWriter CppEnumCodeWriter   Serialization  Basics Current implementation Alternative CppObjectSerializeCodeWriter CppObjectSerializedLengthCodeWriter   Runtime callback handlers  CppObjectDeserializeRuntimeCallbackCodeWriter CppObjectDeserializeFunctionCallbackCodeWriter   See Also    Why code generation? A goal of MLOS is to provide the ability to expose internal application settings, telemetry (metrics, status, etc.), and events to an external agent.\nThis allows observing and tuning the system with minimal impact on its runtime.\nThe MLOS code generation process is responsible for generating a set of classes and helper methods to exchange the messages and share (read and update) config structures between the target application and the external agent. The current mechanism does this using shared memory.\nThe intended purpose of this is to allow the developer to more succinctly specify the settings and some metadata about them (e.g. default and valid range of values) for use in several different domain specific languages (e.g. C++ for systems code, Python for data science, etc.).\nWhy custom CodeGen? Not Protocol Buffers, FlatBuffers, SqlServer XEvents, etc.\nThe answer is performance and full control. Emitting the telemetry will occur on the hot path (not always). We must make sure that we keep code sending the telemetry short and fast as possible. On SKUs with small CPU count, the telemetry code will extend the critical path. The ability to recover the application depends (among others) on the exchange framework.\nAdditionally, some of the existing solutions do not integrate well with some (internal) codebases (e.g. due to use of certain STL features).\nPerformance Interesting benchmarks can be found here bitsery\n   method data size serialize deserialize     bitsery 6913B 1252ms 1170ms   bitsery_compress 4213B 1445ms 1325ms   boost 11037B 9952ms 8767ms   cereal 10413B 6497ms 5470ms   flatbuffers 14924B 6762ms 2173ms   yas 10463B 1352ms 1109ms   yas_compress 7315B 1673ms 1598ms    Then why not XEvents? XEvents generates code responsible for emitting the telemetry; MLOS CodeGen creates both sender and the receiver side. MLOS CodeGen creates additional code required for shared config lookups. MLOS CodeGen also allows annotating attributes with additional metadata for data science purposes.\nBenefits  full control over the code, no external dependencies clean and simple implementation, we are using C# type system (class definitions and attributes) and reflection, simple, extensible architecture to add new language support, just add CodeWriters (for example Python Cpp bindings). easy to integrate with SqlServer (no Cpp standard or open source libraries incompatible with SOS memory management and error handling) easy to open source (no proprietary dependencies)  Build process Codegen works on the types definied in C# assembly. This assembly is called SettingsRegistry. The project definition has a group of files that are included in the code generation workflow.\n\u0026lt;ItemGroup Label=\u0026#34;SettingsRegistryDefs\u0026#34;\u0026gt;\r\u0026lt;!--\rThis one uses an assembly level attribute to declare the output\rnamespace for the code.\r--\u0026gt;\r\u0026lt;SettingsRegistryDef Include=\u0026#34;Codegen/AssemblyInfo.cs\u0026#34; /\u0026gt;\r\u0026lt;!--\rThis one defines the settings and message types.\r--\u0026gt;\r\u0026lt;SettingsRegistryDef Include=\u0026#34;Codegen/SharedChannelConfig.cs\u0026#34; /\u0026gt;\r\u0026lt;/ItemGroup\u0026gt;\r\u0026lt;ItemGroup\u0026gt;\r\u0026lt;!--\rThis one provides the message handler code of those messages for the\rMlos.Agent (C#).\r--\u0026gt;\r\u0026lt;Compile Include=\u0026#34;AssemblyIntializer.cs\u0026#34; /\u0026gt;\r\u0026lt;/ItemGroup\u0026gt;\rFirst codegen internally compiles files marked as SettingsRegistryDef to a temporary assembly. From this assembly, codegen uses reflection to discover all the types that are marked with special C# attributes from Mlos.SettingsSystem.Attributes and process the types though a chain of codegen classes.\nFor instance, a struct may be marked with [CodeGenConfig] (i.e. the CodeGenConfigAttribute) and the settings within it may be marked with [ScalarSetting] (i.e. the ScalarSettingAttribute).\nAs results codegen produces a series of SettingsProvider_gen_*.* files in out/Mlos.CodeGen.out/. We can group them (currently) into two categories:\n  C++ files (.h extension),\n  C# files (.cs extension).\nC# files are compiled with other project files into final settings registry assembly.\n  C++ files are compiled with the client application.\nClasses CodeGen consist multiple CodeWriter types. Each CodeWriter is responsible for providing a small subset of functionality. CodeWriters results are combined into single or multiple files; each CodeWriter specifies the output file as one of its properties.\nHierarchy of CodeWriters Namespaces   Mlos.SettingsSystem.CodeGen.CodeWriters.CppTypesCodeWriters\n CodeGens in this namespace write Cpp struct and proxy view definitions.    Mlos.SettingsSystem.CodeGen.CodeWriters.PythonCodeWriters\n Creates python bindings.    Mlos.SettingsSystem.CodeGen.CodeWriters.CppObjectExchangeCodeWriters\n  Creates set of classes and functions that enables exchange generated objects.\nThis includes helper classes returning type index, serialization and deserialization handlers.\n    Basic structure generation CppObjectCodeWriter Creates a regular C++ structure based on a C# type.\nFrom the following C# code:\nnamespace Mlos.Core.Channel\r{\r/// \u0026lt;summary\u0026gt;\r /// Shared circular buffer channel settings.\r /// \u0026lt;/summary\u0026gt;\r struct ChannelSettings\r{\r/// \u0026lt;summary\u0026gt;\r /// Size of the buffer.\r /// To avoid arithmetic overflow, buffer size must be power of two.\r /// \u0026lt;/summary\u0026gt;\r int BufferSize;\r/// \u0026lt;summary\u0026gt;\r /// Number of reader using this channel.\r /// \u0026lt;/summary\u0026gt;\r int ReaderCount;\r}\r}\rFrom this definition CppObjectCodeWriter generates code:\nnamespace MLOS\r{\rnamespace Core\r{\rnamespace Channel\r{\r// Shared circular buffer channel settings.\r //\r struct ChannelSettings\r{\r// Size of the buffer.\r // To avoid arithmetic overflow, buffer size must be power of two.\r //\r int32_t BufferSize;\r// Number of reader using this channel.\r //\r int32_t ReaderCount;\r};\r}\r}\r}\rNote that the code gen output is using C# namespaces.\nCppProxyCodeWriter Creates a view to a C++ structure. To be more precise, creates a view to serialized form of the structure. Allows to read structure fields after they are serialized in the exchange buffer.\nnamespace Proxy\r{\rnamespace MLOS\r{\rnamespace Core\r{\rnamespace Channel\r{\rstruct ChannelReaderStats : public PropertyProxy\u0026lt;ChannelReaderStats\u0026gt;\r{\rtypedef ::MLOS::Core::Channel::ChannelReaderStats RealObjectType;\rChannelReaderStats(FlatBuffer\u0026amp; flatBuffer, uint32_t offset = 0)\r: PropertyProxy\u0026lt;ChannelReaderStats\u0026gt;(flatBuffer, offset)\r{}\rPropertyProxy\u0026lt;uint64_t\u0026gt; MessagesRead = PropertyProxy\u0026lt;uint64_t\u0026gt;(flatBuffer, offset + 0);\rPropertyProxy\u0026lt;uint64_t\u0026gt; SpinCount = PropertyProxy\u0026lt;uint64_t\u0026gt;(flatBuffer, offset + 8);\r};\r}\r}\r}\r}\rCppProxyCodeWriter creates objects in a Proxy namespace, so we can easily distinguish between the structure and its proxy.\nMLOS::Core::Channel::ChannelReaderStats object;\rProxy::MLOS::Core::Channel::ChannelReaderStats proxyView;\rCppEnumCodeWriter Creates a C++ enums. As enum is a primitive type, there is no proxy class for the enum types.\nenum Colors : ulong\r{\rRed = 2,\rGreen = 5,\rBlue = 9\r};\renum Colors : uint64_t\r{\rRed = 2,\rGreen = 5,\rBlue = 9,\r};\rSerialization Basics Serialization of messages requires a type identifier. The identifier must be the same for the sender and for the receiver. The (C++) sender needs to know the type id during compilation, so the function returning the type identifier must use a constexpr modifier. The receiver uses the type id to find and call proper the handler as a lookup in a dispatch table constructed at compile time. The (C#) receiver does not need to know type identifier during the compilations and dispatch invocation is always dynamic.\nCurrent implementation For each type, CodeGen creates a specialized method TypeMetadataInfo::Index\u0026lt;T\u0026gt; which returns a unique id. To simplify the dispatcher code, ids are simply type indexes.\nnamespace TypeMetadataInfo\r{\rtemplate\u0026lt;typename T\u0026gt;\rstatic constexpr uint32_t Index();\rtemplate \u0026lt;\u0026gt;\rconstexpr uint32_t Index\u0026lt;Point\u0026gt;() { return DispatchTableBaseIndex() + 1; }\rtemplate \u0026lt;\u0026gt;\rconstexpr uint32_t Index\u0026lt;Point3D\u0026gt;() { return DispatchTableBaseIndex() + 2; }\rtemplate \u0026lt;\u0026gt;\rconstexpr uint32_t Index\u0026lt;Line\u0026gt;() { return DispatchTableBaseIndex() + 3; }\r...\r}\rThe CodeGen is creating a dispatcher table:\n__declspec(selectany) ::MLOS::Core::DispatchEntry DispatchTable[] =\r{\r::MLOS::Core::DispatchEntry\r{\rTypeMetadataInfo::Index\u0026lt;::SqlServer::Spatial::Point\u0026gt;(),\r[](/MLOS/source/Mlos.SettingsSystem.CodeGen/FlatBuffer\u0026amp;\u0026amp; buffer)\r{\rProxy::SqlServer::Spatial::Point recvObjectProxy(buffer);\rObjectDeserializationCallback::Deserialize(std::move(recvObjectProxy));\r}\r}\r...\r}\rBecause we are using type indexes, finding a dispatcher routine is a matter of simple table lookup. To verify if the sender and receiver processes are using the same table, the sender will include the hash created from the type definition. That allows receiver to reject invalid/type mismatched frames.\nSpecial steps are required to enable usage of multiple dispatch tables. Each dispatch table must have a unique base index. All type indexes included in this dispatch table will be incremented by the base table index. Base index of next dispatch table is equal to base index of the previous table plus number of types of previous table.\n// Base indexes for all included dispatcher tables.\r//\rconstexpr uint32_t MLOS::Core::Channel::ObjectDeserializationHandler::DispatchTableBaseIndex()\r{\rreturn 0;\r}\rconstexpr uint32_t SqlServer::Spatial::ObjectDeserializationHandler::DispatchTableBaseIndex()\r{\rreturn MLOS::Core::Channel::ObjectDeserializationHandlerHandler::DispatchTableElementCount();\r}\rThe global dispatch table (a concatenation of all included dispatch tables) is created as a compile time constant for efficiency.\nconstexpr auto GlobalDispatchTable()\r{\rauto globalDispatchTable = MLOS::Core::DispatchTable\u0026lt;0\u0026gt;()\r.concatenate(MLOS::Core::Channel::ObjectDeserializationHandler::DispatchTable)\r.concatenate(SqlServer::Spatial::ObjectDeserializationHandler::DispatchTable);\rreturn globalDispatchTable;\r}\rAlternative Use type hash as identifier. Receiver would need to perform a lookup (e.g. hashmap, if/else sequence, binary search, binary search with if/else sequence) to find the correct dispatch function.\nTODO: revisit after implementing C# dispatch handlers\nCppObjectSerializeCodeWriter CppObjectSerializedLengthCodeWriter \u0026hellip; \u0026hellip; TODO \u0026hellip; more documentation needs to go here \u0026hellip;\nRuntime callback handlers Runtime callback handlers allow the developer to dynamically change the callback code.\nCppObjectDeserializeRuntimeCallbackCodeWriter For each type, codegen will generate a callback that can be set in runtime. The root callback namespace is ObjectDeserializationCallback, callbacks for types are created in type specific namespace.\nnamespace ObjectDeserializationCallback\r{\rnamespace _Type_Namespace_\r{\r__declspec(selectany) std::function\u0026lt;void (::Proxy::SqlServer::Spatial::Point\u0026amp;\u0026amp;)\u0026gt; Point_Callback = nullptr;\r__declspec(selectany) std::function\u0026lt;void (::Proxy::SqlServer::Spatial::Point3D\u0026amp;\u0026amp;)\u0026gt; Point3D_Callback = nullptr;\r...\r}\r}\rHere is an example of setting runtime callback, test code is verifying the received object.\nObjectDeserializationCallback::SqlServer::Spatial::Point_Callback = [point](Proxy::SqlServer::Spatial::Point\u0026amp;\u0026amp; recvPoint)\r{\rfloat x = recvPoint.x;\rfloat y = recvPoint.y;\rEXPECT_EQ(point.x, x);\rEXPECT_EQ(point.y, y);\r};\rCppObjectDeserializeFunctionCallbackCodeWriter CodeGen will generate a set of handlers with a default action to call proper callback.\nnamespace ObjectDeserializationCallback\r{\rtemplate \u0026lt;\u0026gt;\rinline void Deserialize\u0026lt;::Proxy::SqlServer::Spatial::Point\u0026gt;(::Proxy::SqlServer::Spatial::Point\u0026amp;\u0026amp; obj)\r{\r::ObjectDeserializationCallback::SqlServer::Spatial::Point_Callback(std::move(obj));\r}\rtemplate \u0026lt;\u0026gt;\rinline void Deserialize\u0026lt;::Proxy::SqlServer::Spatial::Point3D\u0026gt;(::Proxy::SqlServer::Spatial::Point3D\u0026amp;\u0026amp; obj)\r{\r::ObjectDeserializationCallback::SqlServer::Spatial::Point3D_Callback(std::move(obj));\r}\rThere is performance overhead related to a std::function call. However runtime callbacks provide great flexibility and allow runtime changing the callback handlers. Therefore, we envision the primary usage for runtime handlers is testing.\nSee Also  Mlos Settings System Attributes Smart Cache C++ Example  "});index.add({'id':43,'href':'/MLOS/source/Mlos.Streaming/Doc/AggregateStreaming/','title':"Aggregate Streaming",'section':"Sources",'content':"Aggregates on the telemetry streams Intro Processing models     Single return value Multiple return values     Pull/Synchronous/Interactive T IEnumerable\u0026lt;T\u0026gt;   Push/Asynchronous/Reactive Task\u0026lt;T\u0026gt; IObservable\u0026lt;T\u0026gt;    MLOS Telemetry channel Processing events   why not async:\n introduces dedicated processing tasks additional latency introduced by AsyncQueue    why we need a push model\n  Linq operators on observable streams Merging streams operator Collecting the results  in progress  Performance improvements  in progress  "});})();